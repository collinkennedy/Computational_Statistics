{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import statsmodels.api as sm\n",
    "import numpy as np\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import mean_squared_error\n",
    "import pandas as pd\n",
    "from sklearn.metrics import r2_score\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "#import data\n",
    "test_data = pd.read_csv(\"test.data.csv\")\n",
    "train_data = pd.read_csv(\"train.data.csv\")\n",
    "fancy_house = pd.read_csv(\"fancyhouse.csv\")\n",
    "train_data['bed_bath_interaction'] = train_data['bedrooms'] * train_data['bathrooms']\n",
    "test_data['bed_bath_interaction'] = test_data['bedrooms'] * test_data['bathrooms']\n",
    "fancy_house['bed_bath_interaction'] = fancy_house['bedrooms'] * fancy_house['bathrooms']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "#a) R^2 on training data\n",
    "features = ['bedrooms',\"bathrooms\",\"sqft_living\",\"sqft_lot\"]\n",
    "features_w_interaction = ['bedrooms','bathrooms','sqft_living','sqft_lot','bed_bath_interaction']\n",
    "y_train = train_data[[\"price\"]]\n",
    "y_test = test_data[[\"price\"]]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R^2:  0.5101138530794578\n",
      "Test R^2:  0.5049329463821026\n"
     ]
    }
   ],
   "source": [
    "#fit model\n",
    "#standardize the fellas\n",
    "\n",
    "X_train = StandardScaler().fit_transform(train_data[features])\n",
    "X_test = StandardScaler().fit_transform(test_data[features])\n",
    "\n",
    "linear_model = LinearRegression().fit(X = X_train, y = y_train)\n",
    "\n",
    "#generate predictions using train data to calculate training R^2\n",
    "train_preds = linear_model.predict(X_train)\n",
    "test_preds = linear_model.predict(X_test)\n",
    "\n",
    "print(\"Training R^2:  {}\".format(r2_score(y_true = y_train,y_pred=train_preds)))\n",
    "print(\"Test R^2:  {}\".format(r2_score(y_true = y_test, y_pred=test_preds)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>50000</td>\n",
       "      <td>225000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bedrooms  bathrooms  sqft_living  sqft_lot\n",
       "0         8         25        50000    225000"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fancy_house[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Predicted Price of Bill Gate's House:   [15436769.53822249]\n"
     ]
    }
   ],
   "source": [
    "#b)\n",
    "\n",
    "\n",
    "#standardize fancy_house data\n",
    "transformer = StandardScaler()\n",
    "transformer.fit_transform(train_data[features]) #calculate the mean of the train data  and std\n",
    "fancy_scaled = transformer.transform(fancy_house[features]) #use the standardization from the train_data on the fancy_house data\n",
    "\n",
    "#predict price of the house\n",
    "predicted_price = linear_model.predict(fancy_scaled)\n",
    "\n",
    "print(\"Predicted Price of Bill Gate's House:   {}\".format(predicted_price[0]))\n",
    "\n",
    "#\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "15 milly for Bill Gate's compound? Seems  a little low honestly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "New Training R^2:  0.5173532927738305\n",
      "New Test R^2:  0.5105355458590626\n"
     ]
    }
   ],
   "source": [
    "#c)\n",
    "#update data with new variable\n",
    "\n",
    "linear_model = LinearRegression().fit(X = train_data[features_w_interaction], y = y_train)\n",
    "\n",
    "#generate predictions using train data to calculate training R^2\n",
    "train_preds = linear_model.predict(train_data[features_w_interaction])\n",
    "test_preds = linear_model.predict(test_data[features_w_interaction])\n",
    "\n",
    "print(\"New Training R^2:  {}\".format(r2_score(y_true = y_train,y_pred=train_preds)))\n",
    "print(\"New Test R^2:  {}\".format(r2_score(y_true = y_test, y_pred=test_preds)))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part d)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "def rsquared(y_true, y_pred):\n",
    "    #if its not an np.ndarray\n",
    "    if(type(y_true) == pd.DataFrame):\n",
    "        y_true = y_true.to_numpy()\n",
    "    if(type(y_true) == list):\n",
    "        y_true = np.asarray(y_true)\n",
    "    \n",
    "    if(type(y_pred) == pd.DataFrame):\n",
    "        y_pred = y_pred.to_numpy()\n",
    "\n",
    "    if(type(y_pred) == list):\n",
    "        y_pred = np.asarray(y_pred)\n",
    "    res = scipy.stats.linregress(y_true, y_pred)\n",
    "    return res.rvalue**2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.array(pd.DataFrame([1,2,4]))\n",
    "y = np.array(pd.DataFrame([1.2,1.9,3.97]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gd(X,y,learning_parameter = .00001,tau = .001, max_iter = 20000):\n",
    "\n",
    "    n = len(y)\n",
    "    p = X.shape[1]\n",
    "    #intial \"guess\" (first) iteration of betas\n",
    "    beta = np.ones(p)\n",
    "\n",
    "    #reshape so calculations can be performed\n",
    "    beta = beta.reshape(p,1)\n",
    "    y = np.array(y)\n",
    "    \n",
    "    #use this to store SSE\n",
    "    MSE_list = list()\n",
    "    \n",
    "    #standardize  X (seems to help convergence)\n",
    "    standardizer = StandardScaler()\n",
    "    standardizer.fit_transform(X)\n",
    "    X = standardizer.fit_transform(X)\n",
    "\n",
    "    iter_counter = 0 #count number of iterations\n",
    "\n",
    "    while True:\n",
    "        \n",
    "        #calculate the gradient\n",
    "        gradient_vector = X.T @ (X @ beta - y)\n",
    "\n",
    "        #calculate the norm of the vector. Used to determine whether loop should continue\n",
    "        gradient_vector_norm = np.linalg.norm(gradient_vector)\n",
    "\n",
    "        #update beta vector\n",
    "        beta = beta - learning_parameter * gradient_vector \n",
    "\n",
    "        y_pred = X @ beta #calculate predicted values\n",
    "\n",
    "        #calculate current iterations MSE and add to list\n",
    "        MSE = mean_squared_error(y_true = y, y_pred = y_pred)\n",
    "        MSE_list.append(MSE)\n",
    "\n",
    "        iter_counter += 1 #update counter\n",
    "        print('Iteration: {}, Norm of Gradient:  {}'.format(iter_counter,gradient_vector_norm))\n",
    "\n",
    "        if(gradient_vector_norm < tau or iter_counter > max_iter): #if norm of gradient falls below threshold end loop\n",
    "            break\n",
    "    r_squared_calc = rsquared(y.flatten(),y_pred.flatten()) #calculate R-squared\n",
    "    print(\"R-Squared:   {:.5f}\".format(r_squared_calc))   \n",
    "    return(beta,MSE_list, y, y_pred, standardizer)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "beta,returned_MSE_list, y, y_pred, standardizer = gd(train_data[features],y = y_train, learning_parameter = .00004, tau = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training r2\n",
    "training_r2 = rsquared(y_true = y.flatten(), y_pred = y_pred.flatten())\n",
    "\n",
    "#generate predictions\n",
    "X_test = StandardScaler().fit_transform(test_data[features])\n",
    "test_predictions = X_test @ beta\n",
    "\n",
    "test_r2 = rsquared(y_true = y_test.to_numpy().flatten(), y_pred= test_predictions.flatten())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 from Gradient Descent Regression Implementation:   0.51011\n",
      "Testing R2 from Gradient Descent Regression Implementation:   0.50526\n"
     ]
    }
   ],
   "source": [
    "# Repeat a)\n",
    "print(\"Training R2 from Gradient Descent Regression Implementation:   {:.5f}\".format(training_r2))\n",
    "print(\"Testing R2 from Gradient Descent Regression Implementation:   {:.5f}\".format(test_r2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAY4AAAEWCAYAAABxMXBSAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3de1xU1fo/8A9zYxBxQAYFRQEFzB+logJ6zEtpiCdNzVLpIqaHsldWdso8mR4lTx21tK9aeTpmeDmKd1MiVApJTS5DzgwMCoKAQIADyk1QYGbW7w9k6wjDADLMEM/79Xpewdp79jx7O8zTWmtfrAAwEEIIIa3EM3cChBBCuhYqHIQQQtqECgchhJA2ocJBCCGkTahwEEIIaRMqHIQQQtqECgdpFR6Ph6qqKgwYMKBD1yUEANatW4fw8HBzp0FaiQrHn1RVVRUXWq0WNTU13O8vvfRSm7en0+lgZ2eH/Pz8Dl23rSzlC4bP54Mxhj/++AM83v0/I6FQiNLSUtTX13d6TiKRCF9++SUKCgpQWVmJ7OxsfP75552eR0ebPHkytFqt3me6qqoKo0ePNndq3ZbA3AkQ07Czs+N+zsnJwd/+9jf88ssvBtfn8/nQarWdkdqfSlVVFQIDA3Hq1CkAwPTp01FaWgqJRNLpuaxatQrDhg3DqFGjcOPGDbi7u2PcuHGdnocpPkt5eXnw8PAwup6VlRUAgDHWYpsx9PfQMupxdFPr1q3DgQMHsH//flRWVuKVV17BmDFjEB8fj7KyMhQWFmLLli0QCBr+36Lx/7Dd3NwAAHv37sWWLVvw008/obKyEhcvXoS7u3ub1wWAoKAgZGRkoLy8HFu3bsWFCxcQEhLS5n36f//v/yEuLg5lZWVISUnBX//6V27Zs88+i8uXL6OyshL5+flYtmwZAMDJyQlRUVEoKyvDzZs38euvv7bpPffu3YsFCxZwvy9YsAB79uzRW0cikeD7779HYWEh8vPzERYWxn2ZeXp6IjY2FqWlpSgpKcGePXvQq1cv7rX5+fl47733kJKSgvLycuzfvx8ikajZXPz8/HDs2DHcuHEDAJCbm4t9+/Zxy0eOHAm5XI7Kykrs378fhw4dwpo1awAAixcvxtmzZ7l1H/43nDFjBvfa69evY9WqVdy6gwcPBmMMCxcuxPXr13HmzBkAwF/+8hfu8ySXyzF+/HjuNR4eHjh37hwqKytx6tQpODo6tuGo6zt//jw++eQTXLx4EdXV1Rg4cGCzbf3790dkZCRu3ryJq1ev4rXXXuO20dzfA2kZo/hzR05ODps8ebJe27p161htbS2bPn06s7KyYmKxmI0ePZr5+/szPp/PPDw8WEZGBnvrrbcYAMbn8xljjLm5uTEAbO/evaykpISNGjWKCQQCduDAAbZ37942r+vk5MQqKyvZc889xwQCAXvvvfdYXV0dCwkJaXZf1q1bx8LDw5u0C4VClp2dzZYvX84EAgGbPHkyq6qqYoMHD2YAmFqtZmPHjmUAmIODA/P19WUA2MaNG9m2bduYQCBgQqGQTZgwgdvmf/7zH7Zly5Zm82jcx6FDh7Li4mJmZ2fHevfuzYqKitiwYcNYfX09t25kZCT7+uuvmY2NDevbty9LTk5mixYtYgCYl5cXe/rpp5lQKGROTk7swoUL7PPPP+dem5+fz+Lj41nfvn1Z7969WUZGBlu8eHGzOa1Zs4bl5uayJUuWMB8fH71lIpGI5efns7fffpsJBAI2b948VldXx9asWcMAsMWLF7OzZ8822b/Gf8OnnnqK+fj4MCsrKzZs2DBWUlLCnn32WQaADR48mDHG2Pfff89sbGyYWCxmrq6urLS0lAUGBjIrKys2depUVlJSwnr37s0AsMTERLZx40YmEonYpEmTWFVVVbP/rgDY5MmTWU5OjsHP9/nz51lOTg577LHHmEAgYHw+v9m2CxcusK1btzJra2vm6+vLSkpKuH/v5v4ezP13a+Fh9gQ6JXbu3Mlu3LjBUlNTja47fvx49vvvv7P6+no2Z84cvWXR0dGsrKyMRUZGmn2fWhuGCscvv/zS4uvef/99dujQIQY0Xwy2b9/OrTtjxgzu2LZl3ddee42dO3dO730LCwvbXDgmTZrECgoK9NoOHTrEPv74YwaA/fHHH2zx4sWsZ8+eeut8+umn7OjRo2zQoEFtOqYP7mN4eDhbtGgRe+utt9g333zDhgwZwhWOfv36sZqaGiYSibjXvvLKK+zMmTPNbnfOnDksKSmJ+z0/P5/NmzeP+33Tpk1s27ZtBnNaunQp++2339jdu3dZQUEBe/nllxnQ8MWfl5ent35iYmKrC8fDsW3bNrZx40YG3C8cAwYM4JavXLmSff/993qv+fnnn9lLL73EPDw8WG1tLbOxseGWHTx4sMXCodVqWVlZmV40HtPz58+z1atX673m4TZ3d3dWV1fHevTowbVt3LiR7dixo9V/DxT3o9sMVe3atQtBQUGtWjcvLw8LFy7E/v37myz7/PPP8eqrr3Z0embx8OT1kCFD8OOPP6KoqAgVFRX45JNPIJVKDb6+uLiY+7mmpgY9e/Zs87r9+vVrkkdBQUGb9qNxO3l5eXpt169fR//+/QEAs2fPxnPPPYe8vDycPXsW/v7+AID169fj+vXr+OWXX5CVlYUPPvigze+9Z88eLFiwoNlhKjc3N1hbW+PGjRsoKytDWVkZvv76a/Tt2xcA0LdvXxw8eBAFBQWoqKjArl27mhzz1h5nrVaLr776CuPGjYO9vT02btyIXbt2wcvLC/369WtyXK9fv97qfRwzZgzOnj0LtVqN8vJy/O1vf2uS54P/jm5ubggODub2uaysDGPGjEG/fv3Qr18/3Lx5E3fu3Gl1Lnl5eXBwcNCLurq6Zt+7ubZ+/fqhtLQUNTU1eu/Z+PkwtA3SvG5TOM6fP49bt27ptQ0aNAjR0dFITk7GuXPnMGTIEAANH6jU1FTodLom24mNjUVVVVWn5GxqD08Wfvvtt1CpVPD09IREIsE///lPbizeVIqKiuDq6qrX9uAfc2sVFhY2Of134MCB+OOPPwAASUlJmDlzJvr06YMff/wRBw4cANAwuf33v/8dHh4emDVrFlasWIEJEya06b3Pnj0LNzc32NvbIyEhQW9Zfn4+ampq0Lt3b+4LTyKRYPjw4QCADRs2oLa2Fk888QQkEgkWLlzYIcf87t272Lp1K27fvo2hQ4c2e5wHDhzI/VxdXY0ePXpwvzs7O+ute+DAARw9ehQDBgyAvb09vvvuuxbzzM/PR3h4uN4Xfc+ePfHFF1+gqKgIjo6OEIvFzebSHs1NfD/YVlhYCKlUqrePD34+DG2DNK/bFI7m/Pe//8Xbb7+N0aNH44MPPsA333xj7pTMys7ODhUVFaiursZjjz2GN954w+Tv+eOPP2LkyJGYPn06+Hw+3n33XTg5ObX4Gj6fD2tray5EIhEuXrwIjUaD999/HwKBAE899RT++te/4tChQxCLxQgODoadnR00Gg13ijLQcBbUoEGDAAAVFRXQarXtOptm+vTpmDVrVpP2goIC/Prrr/jiiy9gZ2cHKysrDB48mJsotrOzQ3V1NSoqKuDq6tquHk+jZcuWYfz48RCLxeDz+XjttdcgFouhUChw4cIF8Hg8vPXWW+Dz+XjhhRcwcuRI7rVKpRLDhg3D448/DrFYzE2aN7Kzs8OtW7dQW1uLgIAAzJ8/v8Vc9u7di9mzZ2PKlCng8XiwtrbGpEmT4OLiguzsbKSkpGDt2rUQCoUYP348nn322Xbvd2vk5uYiOTkZn332GUQiEYYPH47XXntN7+QB0nrdtnDY2triL3/5Cw4fPgy5XI5vv/0WLi4u5k7LrN5//32EhISgqqoK3377LQ4ePGjy91Sr1Zg3bx42b96MmzdvYvDgwZDL5aitrTX4mldffRV3797lIiMjA3V1dZgxYwZmzpyJ0tJSbN26FS+99BIyMzMBACEhIbh+/ToqKiqwePFibrhxyJAhiI2Nxe3bt/Hbb79hy5Yt+O233wAAO3bswLZt21q1H2lpabhy5Uqzy1555RXY2tri8uXLKCsrw+HDh7n/o1+zZg38/f1RUVGBkydP4ujRo60+dg+7e/cu/u///g83btxAaWkp3njjDTz//PPIy8tDXV0dZs+ejdDQUJSVleH555/HDz/8wL32ypUr+OyzzxAXF4eMjAycO3dOb9tvvvkm/v3vf6OyshIrV67EoUOHWszl+vXrmD17NlavXo2SkhLk5eXh/fff5655mT9/PsaNG4dbt27h448/xt69e1vc3sCBA5tcxzFz5sw2HZ958+bBy8sLxcXFOHLkCFauXIm4uLg2bYPcZ/aJls4KNzc3blLWzs6OFRYWtrh+eHh4k8lxAGzixIldanK8KwWPx2PFxcXsySefNHsuf/bYu3cvNzlOQdGW6LY9jqqqKuTk5OCFF17g2oYNG2bGjLqvqVOnolevXhCJRFi9ejU0Gg2SkpLMnRYhpAVmr16dEfv372eFhYWsrq6O5efns0WLFjF3d3cWHR3NFAoFS0tL407fGz16NMvPz2e3b99mpaWlTKVScds5d+4cU6vVrKamhuXn57PAwECz71tXj3Xr1rHS0lJWWVnJ4uPj2ejRo82eU3cI6nFQtDes7v1ACCGEtEq3HaoihBDSPt3iJodqtbpNFzsRQghpuJCzT58+Tdq7ReG4fv06/Pz8zJ0GIYR0KTKZrNl2GqoihBDSJlQ4CCGEtAkVDkIIIW1issJhbW2NxMREKBQKqFQqrF27tsk6mzdvhlwuh1wuR0ZGBsrKyrhlGo2GW3bixAmu3d3dHQkJCbh69SoOHDgAoVBoql0ghBBigMkuErG1tWUAmEAgYAkJCSwgIMDgukuXLmU7d+7kfq+qqmp2vYMHD3LPJ9i+fTtbsmSJ0TxkMpnZL5ihoKCg6Gph6LvTpENV1dXVAAChUAihUNjibYuDg4MRERFhdJtPP/00jhw5AgDYvXt3s3ckJYQQYjomLRw8Hg9yuRxqtRoxMTEG7z80cOBAeHh4IDY2lmsTi8WQyWSIj4/n7oLp6OiI8vJy7rbXBQUFBp/dEBoaCplMBplM1uLDiAghhLSdybs7EomExcbGNnkOcmN8+OGHbOvWrXptLi4uDADz8PBgOTk5bNCgQUwqlbLMzExuHVdXV5aSktLu7paxGDl9Khv74myzdxcpKCgozBFmGapqVFFRgbi4OIOPbp0/f36TYaqioiIAQE5ODuLi4uDr64vS0lLY29uDz+cDAFxdXVFYWGiyvH2DnkHAnBkm2z4hhHRFJiscUqkUEokEQMOw05QpU5Cent5kPW9vbzg4OCA+Pp5rs7e3h0gkAtAwPDVu3DhcvnwZQMNjOhtvhR4SEqJ3xlVH02k14Au6xcX1hBDSaiYrHC4uLjh79iyUSiVkMhliYmIQFRWFsLAwzJhx///ig4ODuec/Nxo6dCiSk5OhUChw9uxZrF+/nnu62ooVK/D3v/8dmZmZcHR0xM6dO021C9BqtODd690QQgi5z+zjaKaO9s5xvLIhjP0j8qDZ86egoKAwR5h1jqOr0mq04NFQFSGE6KHC0QKdVgu+gIaqCCHkQVQ4WqDVaKjHQQghD6HC0QKtRsOd+ksIIaQBFY4W6GiOgxBCmqDC0QKtRkOn4xJCyEOocLRAp9WCL6QeByGEPIgKRwuox0EIIU1R4WiBTqsFj8eDFY8OEyGENKJvxBZo6zUAQL0OQgh5ABWOFujuPfeDbnRICCH3UeFogVZzr8dBV48TQgiHCkcLqMdBCCFNUeFoAdfjoDkOQgjhUOFoge7e5Dj1OAgh5D4qHC3Q0lAVIYQ0QYWjBTqaHCeEkCZMVjisra2RmJgIhUIBlUqFtWvXNlln8+bNkMvlkMvlyMjIQFlZGQBg+PDhuHjxIlQqFZRKJebOncu9Jjw8HNnZ2dzrhg8fbqpdoB4HIYQYYLLHDtra2jIATCAQsISEBBYQEGBw3aVLl7KdO3cyAMzLy4t5enoyAMzFxYUVFhYyiUTCALDw8HA2Z86cDnn8obF4/OkJbFNqPOs3xMvsj3CkoKCg6Owwy6Njq6urAQBCoRBCoRCMMYPrBgcHIyIiAgCQmZmJrKwsAEBRURHUajWcnJxMmWqztBrqcRBCyMNMWjh4PB7kcjnUajViYmKQlJTU7HoDBw6Eh4cHYmNjmyzz8/ODSCTCtWvXuLZPP/0USqUSmzdvhkgkanaboaGhkMlkkMlkkEql7cqf5jgIIaR5Ju/uSCQSFhsby3x8fJpd/uGHH7KtW7c2aXd2dmbp6el6Q1zOzs4MABOJRGzXrl1s9erV7e5uGQuvgNFsU2o8GzRqhNm7jBQUFBSdHWYZqmpUUVGBuLg4BAUFNbt8/vz53DBVIzs7O0RFRWHVqlVITEzk2ouLiwEAdXV1CA8Ph7+/v8nypgsACSGkKZMVDqlUColEAgAQi8WYMmUK0tPTm6zn7e0NBwcHxMfHc21CoRDHjx/Hnj17cOTIEb31nZ2duZ9nzZoFlUploj1oeHQsQHMchBDyIJN9I7q4uGD37t3g8/ng8Xg4dOgQoqKiEBYWhuTkZERGRgJomBQ/cOCA3mvnzp2LCRMmwNHREQsXLgQALFy4EEqlEvv27YOTkxOsrKygUCiwZMkSU+0C9TgIIcQAs4+jmTraO8fRb4gX25Qazx5/eoLZ94GCgoKis8OscxxdVePdcanHQQgh91HhaEHjUBXNcRBCyH1UOFrQODnOo8JBCCEcKhwt4HocNFRFCCEcKhwt4OY4hNTjIISQRlQ4WkA9DkIIaYoKRwu4HgfNcRBCCIcKRwu09dTjIISQh1HhaAH1OAghpCkqHC3Q0m3VCSGkCSocLWA6HQC6AJAQQh5EhcMITX093XKEEEIeQIXDCG29hnochBDyACocRui0WprjIISQB1DhMEKnoR4HIYQ8iAqHEVqtluY4CCHkAVQ4jKAeByGE6DNZ4bC2tkZiYiIUCgVUKhXWrl3bZJ3NmzdDLpdDLpcjIyMDZWVl3LIFCxbg6tWruHr1KhYsWMC1jxw5EikpKcjMzMSWLVtMlT5Hq9FS4SCEkIeY7LGDtra2DAATCAQsISGBBQQEGFx36dKlbOfOnQwAc3BwYNeuXWMODg7M3t6eXbt2jdnb2zMALDExkY0ZM4YBYD/99BMLCgpq9+MPWxP/iDzIXt4QZvZHOFJQUFB0dpjl0bHV1dUAAKFQCKFQCMaYwXWDg4MREREBAJg6dSpiYmJQVlaG8vJyxMTEICgoCM7OzujVqxcSEhIAAHv27MGsWbNMuQvQaqnHQQghDzJp4eDxeJDL5VCr1YiJiUFSUlKz6w0cOBAeHh6IjY0FAPTv3x/5+fnc8oKCAvTv3x/9+/dHQUFBk/bmhIaGQiaTQSaTQSqVtnsfdBoNTY4TQsgDWiwcPB4PMTEx7d64TqeDr68vXF1d4e/vDx8fn2bXmz9/Po4cOQLdvVt8WFlZNVmHMWawvTk7duyAn58f/Pz8UFpa2u59oB4HIYToa7Fw6HQ61NTUoFevXo/0JhUVFYiLi0NQUFCzy+fPn88NUwENPYkBAwZwv7u6uqKwsBAFBQVwdXVt0m5KunrqcRBCyIOMDlXdvXsXqamp+O6777BlyxYujJFKpZBIJAAAsViMKVOmID09vcl63t7ecHBwQHx8PNd2+vRpBAYGwt7eHvb29ggMDMTp06dRXFyMqqoqBAQEAGg48+rEiROt3tn20NLpuIQQosfoN2JUVBSioqLavGEXFxfs3r0bfD4fPB4Phw4dQlRUFMLCwpCcnIzIyEgADZPiBw4c0HttWVkZ1q1bB5lMBgD45JNPuFN133zzTezatQs2NjaIjo5GdHR0m3NrC7rlCCGE6LNCw+lVLRIKhfD29gYAZGRkQHPvORVdhUwmg5+fX7te+/q3/wfrHj2w7dXXOzgrQgixbIa+O432OCZOnIjdu3cjNzcXVlZWGDBgAEJCQnD+/HmTJGppdHTLEUII0WO0cGzatAmBgYG4evUqAMDLywsREREYPXq0yZOzBDTHQQgh+oxOjguFQq5oAEBmZiaEQqFJk7IkOg3NcRBCyIOM/q90cnIyvvvuO+zduxcA8PLLL+P33383eWKWgnochBCiz+g34ptvvom33noL77zzDqysrHDu3Dl88803nZGbRaCzqgghRF+LhYPH42Hnzp149dVX8eWXX3ZWThZFS7ccIYQQPUavHHdycupWcxoP09Ft1QkhRI/Rb8Tc3Fz89ttvOHnyJHe3WwDdpgdCPQ5CCNFntHAUFhaisLAQPB4PdnZ2nZGTRdFqNOALqcdBCCGNjM5x9OzZEx9++GFn5WNx6AJAQgjRZ3SOY+TIkZ2Vi0WiZ44TQog+o9+ICoUCJ06cwOHDh/XmOI4fP27SxCyFlnochBCix2jh6N27N27evImnn36aa2OMdZvCoaunHgchhDzI6DfiokWLOiMPi6XVagEAPAEfOo3WzNkQQoj5GZzjOHjwIPfz+vXr9ZadPn3adBlZGN29W8hTr4MQQhoYLBxeXl7cz88884zeMicnJ9NlZGG4HgfNcxBCCIAWCgdjhp/v1NKyPxvqcRBCiD6DhaNHjx4YMWIERo4cCRsbG4wYMQK+vr7c78ZYW1sjMTERCoUCKpUKa9eubXa9F198EWlpaVCpVNi3bx8AYNKkSZDL5VzcuXMHM2fOBACEh4cjOzubWzZ8+PB27HbraTX35zgIIYS0MDleVFSEzZs3AwCKi4u5nxt/N6a2thZPP/00qqurIRAIcOHCBURHRyMxMZFbx9PTEx999BHGjRuH8vJybggsLi4Ovr6+AAAHBwdkZWXhzJkz3OuWL1+Oo0ePtnFX24d6HIQQos/gt+GDp9+2V+N1H0KhEEKhsMkQV2hoKL7++muUl5cDAEpKSpps44UXXkB0dDTu3LnzyPm0R2OPgwoHIYQ0MPoEwEfaOI8HuVwOtVqNmJgYJCUl6S339vaGt7c3Lly4gPj4eEydOrXJNubPn4+IiAi9tk8//RRKpRKbN2+GSCRq9r1DQ0Mhk8kgk8kglUrbvQ86bUOPgybHCSHkPmbqkEgkLDY2lvn4+Oi1R0ZGsmPHjjGBQMDc3d1Zfn4+k0gk3HJnZ2emVquZQCDQawPARCIR27VrF1u9erXR95fJZO3OffjUyWxTajzrO8jd5MeJgoKCwpLC0HenSXscjSoqKhAXF4egoCC99oKCApw4cQIajQa5ubnIyMjQOw147ty5OH78ODT35hmA+/MrdXV1CA8Ph7+/v0lzb5zjoMlxQghpYHDgvnFy2hC5XN7icqlUivr6elRUVEAsFmPKlCnYsGGD3jo//PADgoODsXv3bjg6OsLb2xvZ2dnc8uDgYHz00Ud6r3F2duaKx6xZs6BSqVrM41HRHAchhOgz+G24adMmAIBYLMbo0aOhVCphZWWFYcOGITExEePHj29xwy4uLti9ezf4fD54PB4OHTqEqKgohIWFITk5GZGRkTh9+jQCAwORlpYGrVaL5cuX49atWwAANzc3DBgwAL/++qvedvft2wcnJydYWVlBoVBgyZIlj3oMWkRzHIQQ0lSLY1wRERHs8ccf53738fFh4eHhZh97a0s8yhyH1xg/tik1nnn4DjP7flBQUFB0ZrR7juOxxx7TGw5KS0vDiBEjjL3sT4Ob46AeByGEAGjF3XGvXLmCHTt24H//+x8YY3jllVdw5cqVzsjNInBzHPT4WEIIAdCKwvHaa6/hzTffxLvvvgsAOHfuHLZv327yxCwFzXEQQog+o4WjtrYW//nPf/DTTz/h6tWrnZGTRdHSLUcIIUSP0TmOGTNmQKFQ4NSpUwCA4cOH48SJEyZPzFJwNzmkHgchhABoReFYs2YN/P39uftJKZVKuLu7mzovi0E3OSSEEH1GC4dGo0FlZWVn5GKRuB4HTY4TQgiAVhQOlUqF4OBg8Pl8eHp6YuvWrbh48WJn5GYRGifH+TRURQghAFpRON5++234+PigtrYW+/fvR0VFBZYtW9YZuVmE+w9yoh4HIYQARs6q4vF4CAsLw4cffohVq1Z1Vk4WhU7HJYQQfS32OHQ6HUaNGtVZuVgkbT3d5JAQQh5k9NtQLpfjxIkTOHz4MPdEPwA4fvy4SROzFFyPg26rTgghAFpROHr37o2bN2/qPUqWMdZtCgfdVp0QQvQZ/TZctGhRZ+RhsWiOgxBC9BktHNbW1li8eDF8fHwgFou59sWLF5s0MUuhox4HIYToMXo67t69e+Hs7IypU6fi119/haurK6qqqjojN4uhrdfQHAchhNxjtHB4enrin//8J6qrq7Fnzx48++yzeOKJJzojN4uh02qpx0EIIfcYLRz19fUAgPLycvj4+EAikbTqXlXW1tZITEyEQqGASqXC2rVrm13vxRdfRFpaGlQqFfbt28e1azQayOVy7qyuRu7u7khISMDVq1dx4MABCIVCo7k8Kq1GQ3MchBDygBYfHbh48WJmb2/PJkyYwK5du8Zu3LjB3njjjVY9dtDW1pYBYAKBgCUkJLCAgAC95Z6enuzSpUvM3t6eAWBOTk7csqqqqma3efDgQTZv3jwGgG3fvp0tWbKk3Y8/bG2su3CazfrHe2Z/jCMFBQVFZ0YL352mf3MbGxv2+++/M39/f732DRs2sMWLFzf7GkOFo6SkhPH5fAaAjRkzhp06depRdr5VsTYuij3/8Qdm/0ekoKCg6Mww9N1pdOB+9erVzbavW7fO2EvB4/Hw+++/w9PTE19//TWSkpL0lnt7ewMALly4AD6fj7Vr1+L06dMAALFYDJlMBo1Gg/Xr1+PEiRNwdHREeXk5tNqGM50KCgrQv3//Zt87NDQUr7/+OgBAKpUazbUlOg3NcRBCSCOj34YPXi0uFosxffr0Vj9zXKfTwdfXFxKJBMePH4ePjw/S0tLuv7lAAC8vL0yaNAmurq44f/48Hn/8cVRUVGDgwIEoKiqCh4cHYmNjkZqa2uzt3Rljzb73jh07sGPHDgCATCZrVb6GaOrrwe+EuRRCCOkKjBaOzZs36/3+xRdf4OTJk216k4qKCsTFxSEoKEPKskYAAB34SURBVEivcBQUFCAhIQEajQa5ubnIyMiAl5cXkpOTUVRUBADIyclBXFwcfH19cfToUdjb24PP50Or1cLV1RWFhYVtyqU96u7cgXUPG5O/DyGEdAVGz6p6WI8ePTBo0CCj60mlUkgkEgANPZUpU6YgPT1db50ffvgBTz31FADA0dER3t7eyM7Ohr29PUQiEdc+btw4XL58GQBw9uxZvPDCCwCAkJCQTnmMbW11Daxte5j8fQghpCsw2uNISUnhhoP4fD6cnJzwySefGN2wi4sLdu/eDT6fDx6Ph0OHDiEqKgphYWFITk5GZGQkTp8+jcDAQKSlpUGr1WL58uW4desWxo4di2+//RY6nQ48Hg/r16/nhsdWrFiBAwcO4F//+hfkcjl27tz5iIfAuNrqalj3tDX5+xBCSFdghYZZcoMGDhzI/azRaHDjxg1ucrqrkMlk8PPza/frF2z6FH0HuePz2S93YFaEEGLZDH13Gu1xPHx7kV69eun9XlZW9oipWT4aqiKEkPuMFo5Lly5hwIABKCsrg5WVFezt7ZGXlweg4YymwYMHmzxJc7tbXU2FgxBC7jE6OX7q1CnMmDEDTk5OkEqlmD59Oo4dO4ZBgwZ1i6IBALU1NRDb0hwHIYQArSgcfn5+iI6O5n4/deoUJk6caNKkLE3t7Wrw+HwIxdbmToUQQszOaOEoLS3Fxx9/DDc3NwwcOBArV67EzZs3OyM3i3G3ugYAaLiKEELQisIRHBwMJycnHD9+HD/88AP69OmD4ODgzsjNYtTWNBQOGq4ihJBWTI6XlZVh2bJlAAB7e3uUl5ebPClLU3u74bYr1OMghJAWehyrV6/GkCFDAAAikQi//PILsrKycOPGDUyePLnTErQEjUNV1OMghJAWCse8efOQkZEBoOHWHjweD3369MHEiRPx2WefdVqClqCWm+OgwkEIIQYLR11dHffz1KlTERERAZ1Oh/T0dAi62S3G71bTUBUhhDQyWDhqa2vh4+MDqVSKp556CmfOnOGW9ejRvb5Aa2moihBCOAa7Du+++y6OHDkCJycnfPnll8jNzQUATJs2DXK5vLPyswi1dDouIYRwDBaOpKQkDB06tEl7dHS03gWB3UHdnTvQ6XRUOAghBO14Hkd3VVtNtx0hhBCACker1dbQHXIJIQSgwtFqd29XQ0wPcyKEEONXjgPA2LFj4e7urnca7t69e02WlCWqra6BdTc7m4wQQppjtMexZ88efPHFF3jyySfh5+cHPz8/jB492uiGra2tkZiYCIVCAZVKhbVr1za73osvvoi0tDSoVCrs27cPADB8+HBcvHgRKpUKSqUSc+fO5dYPDw9HdnY25HI55HI5hg8f3spdfTQ0VEUIIfexluLy5cstLm8pbG1tGQAmEAhYQkICCwgI0Fvu6enJLl26xOzt7RkA5uTkxAAwLy8v5unpyQAwFxcXVlhYyCQSCQPAwsPD2Zw5c9qUh0wma/c+NEbIl/9mHxz73yNvh4KCgqKrhKHvTqM9DpVKBWdnZ2OrNav63hXXQqEQQqEQjDG95aGhofj666+5GyeWlJQAADIzM5GVlQUAKCoqglqthpOTU7ty6Cg0VEUIIQ2MFg6pVIrLly/j1KlTOHHiBBet2jiPB7lcDrVajZiYGCQlJekt9/b2hre3Ny5cuID4+HhMnTq1yTb8/PwgEolw7do1ru3TTz+FUqnE5s2bIRKJmn3v0NBQyGQyyGQySKXSVuXbEhqqIoSQ+1rsqkyYMKHZMPa6B0MikbDY2Fjm4+Oj1x4ZGcmOHTvGBAIBc3d3Z/n5+dyQFADm7OzM0tPT9Ya4nJ2dGQAmEonYrl272OrVq9vd3WpLTHtnCdt46bzZu44UFBQUnRWGvjuNnlV17tw5Y6sYVVFRgbi4OAQFBSEtLY1rLygoQEJCAjQaDXJzc5GRkQEvLy8kJyfDzs4OUVFRWLVqFRITE7nXFBcXA2i4CWN4eDg++OCDR86vNWqra8AXCiAQiaB54AaQhBDS3RgdqgoICEBSUhKqqqpQW1sLjUaDiooKoxuWSqWQSCQAALFYjClTpiA9PV1vnR9++AFPPfUUAMDR0RHe3t7Izs6GUCjE8ePHsWfPHhw5ckTvNQ/Ot8yaNQsqlcr4XnaA2nvzNXQtByGkuzPa4/jqq68wf/58HD58GKNHj8aCBQvg5eVldMMuLi7YvXs3+Hw+eDweDh06hKioKISFhSE5ORmRkZE4ffo0AgMDkZaWBq1Wi+XLl+PWrVt4+eWXMWHCBDg6OmLhwoUAgIULF0KpVGLfvn1wcnKClZUVFAoFlixZ8sgHoTW454736IHbt8o65T0JIcRStWqMS6lUcm2//fab2cfe2hIdMcfx+NMT2abUeNZviJfZ94eCgoKiM6Ldcxw1NTUQCoVQKBTYsGEDioqKYNsNb/ZHQ1WEENLA6BzHq6++Ch6Ph6VLl6K6uhoDBgzAnDlzOiM3i/LgUBUhhHRnRnsceXl5EIvFcHFxwSeffNIZOVmkWnp8LCGEAGhFj2P69OlQKBQ4deoUgIb7SLX2AsA/k8YeBw1VEUK6O6OFY+3atfD39+duC6JUKuHu7m7qvCxO7e17cxzdcH6HEEIeZLRwaDQaVFZWdkYuFq22pgb1tbXo6djb3KkQQohZteomh8HBweDz+fD09MTWrVtx8eLFzsjN4lSWlKKXk6O50yCEELMyWjjefvtt+Pj4oLa2FhEREaisrMSyZcs6IzeLU1lyE72cHv2GiYQQ0pUZPavqzp07WLVqFVatWtUZ+Vi0ypJSOHsOMncahBBiVgYLh7Ezp2bOnNnhyVi6ypJSeI/xM3cahBBiVgYLx9ixY5Gfn4+IiAgkJibCysqqM/OySJUlN2HTyw5CsTXq79aaOx1CCDELg4XD2dkZzzzzDIKDg/HSSy8hKioKERERuHz5cmfmZ1GqSksBAL2kUtws+MPM2RBCiHkYnBzX6XQ4ffo0Fi5ciDFjxiArKwtxcXFYunRpZ+ZnUSrU9woHnVlFCOnGWpwcF4lEePbZZxEcHAx3d3ds3boVx44d66zcLE5l6U0AQK8+5n3+OSGEmJPBwrFr1y48/vjjiI6ORlhYmN6T+7qrqpLGoSrqcRBCui+DhePVV19FdXU1vL298c4773DtVlZWYIxxT/frTqrLK6Cpr6ehKkJIt2awcPD5/M7Mo8touHqchqoIId2X0SvH28va2hqJiYlQKBRQqVRYu3Zts+u9+OKLSEtLg0qlwr59+7j2BQsW4OrVq7h69SoWLFjAtY8cORIpKSnIzMzEli1bTJW+QVUlN6nHQQjp9kz22EFbW9uGxwwKBCwhIYEFBAToLff09GSXLl1i9vb2DABzcnJiAJiDgwO7du0ac3BwYPb29uzatWvcOomJiWzMmDEMAPvpp59YUFBQux9/2J4I+fLf7INj/zP7Ix0pKCgoTB2GvjtN1uMAgOp7Dz8SCoUQCoVgjOktDw0Nxddff83dsr2kpAQAMHXqVMTExKCsrAzl5eWIiYlBUFAQnJ2d0atXLyQkJAAA9uzZg1mzZplyF5qoKr0JCZ1VRQjpxkxaOHg8HuRyOdRqNWJiYpCUlKS33NvbG97e3rhw4QLi4+MxdepUAED//v2Rn5/PrVdQUID+/fujf//+KCgoaNLenNDQUMhkMshkMkilHXdjwgp1CXpIekEgEnXYNgkhpCsxaeHQ6XTw9fWFq6sr/P394ePjo7dcIBDAy8sLkyZNQnBwML777jtIJJJmb2/CGDPY3pwdO3bAz88Pfn5+KL13xXdHqCppuJbDTkrP5SCEdE8mLRyNKioqEBcXh6CgIL32goICnDhxAhqNBrm5ucjIyICXlxcKCgowYMAAbj1XV1cUFhaioKAArq6uTdo7U+W9IiShM6sIId2UyQqHVCrlrvUQi8WYMmUK0tPT9db54Ycf8NRTTwEAHB0d4e3tjezsbJw+fRqBgYGwt7eHvb09AgMDcfr0aRQXF6OqqgoBAQEAGs686uznn5cXqwEADv2cO/V9CSHEUhh9Hkd7ubi4YPfu3eDz+eDxeDh06BCioqIQFhaG5ORkREZGcgUiLS0NWq0Wy5cvx61btwAA69atg0wmAwB88sknKCsrAwC8+eab2LVrF2xsbBAdHY3o6GhT7UKzSvMKoNPp4OQ+sFPflxBCLInZT/kydXTk6bgA2MroI+yVDWFm3y8KCgoKU4ZZTsf9s1LnXEcfD3dzp0EIIWZBhaMd1DnXIXUbQA+3IoR0S1Q42kGdmwfrHjaQ9O1j7lQIIaTTUeFoB3XOdQBAHw83M2dCCCGdjwpHO6hzcgEAfTzozCpCSPdDhaMdbt8sw53KKpogJ4R0S1Q42kmdcx193GmoihDS/VDhaCd17nU40VAVIaQbosLRTuqcPNj37QPrHj3MnQohhHQqKhztVHQ1CwDQf6i3mTMhhJDORYWjna6nqAAA7iOeMHMmhBDSuahwtFNNRSVuZOfCfTgVDkJI90KF4xHkKlKpx0EI6XaocDyCXEUqbB3s6RbrhJBuhQrHI8hVpACgeQ5CSPdCheMRlOTmobq8guY5CCHdChWOR8AYw3WlCu6+w8ydCiGEdBqTFQ5ra2skJiZCoVBApVJh7dq1TdYJCQmBWq2GXC6HXC7H4sWLAQCTJk3i2uRyOe7cuYOZM2cCAMLDw5Gdnc0tGz58uKl2oVWuJcvhPNgD9s59zZoHIYR0JpM9dtDW1pYBYAKBgCUkJLCAgAC95SEhIWzbtm0tbsPBwYHdvHmT2djYMAAsPDyczZkzp0Mef9gR4TjAlW1KjWfjX5ln9sc8UlBQUHRkmOXRsdXV1QAAoVAIoVAIxlibt/HCCy8gOjoad+7c6ej0OsTN/AIUZmRi2JRJ5k6FEEI6hUkLB4/Hg1wuh1qtRkxMDJKSkpqsM2fOHCiVShw+fBiurq5Nls+fPx8RERF6bZ9++imUSiU2b94MkUjU7HuHhoZCJpNBJpNBKpV2zA4ZkPpzHNx9h8HOsbdJ34cQQiyFybs7EomExcbGMh8fH7323r17M5FIxACwN954g/3yyy96y52dnZlarWYCgUCvDQATiURs165dbPXq1e3ubnVUOHsOYptS49nYF2ebvWtJQUFB0VFhlqGqRhUVFYiLi0NQUJBe+61bt1BXVwcA2LFjB0aNGqW3fO7cuTh+/Dg0Gg3XVlxcDACoq6tDeHg4/P39TZy9ccVZ2VDnXMeIoMnmToUQQkzOZIVDKpVCIpEAAMRiMaZMmYL09HS9dZydnbmfn3vuOVy5ckVveXBwcJNhqgdfM2vWLKhUqo5OvV1kJ36Cp/8ouHgPNncqhBBiUiYrHC4uLjh79iyUSiVkMhliYmIQFRWFsLAwzJgxAwDwzjvvQKVSQaFQ4J133sHChQu517u5uWHAgAH49ddf9ba7b98+pKSkIDU1FVKpFP/6179MtQttEn/4B9TW3MHEBcHmToUQQkzO7ONopg5Tz3E0xqx/vMc2XDrHejlJzb7PFBQUFI8aZp3j6C7O/e8geDwe9ToIIX9qVDg60K2CQiSfjMb4l+ei72APc6dDCCEmQYWjg/24+SvcvX0bL6z+EFZWVuZOhxBCOhwVjg5WXV6ByE3bMGjUCDz58lxzp0MIIR2OCocJyE78BFXsr3jug7cxdPxfzJ0OIYR0KCocJrLvH2vxR0YmXv1iHTxGmvcOvoQQ0pGocJhI3Z27+H7pclTcKMGSHVsx8tlAc6dECCEdggqHCVWWlGLrK68jV6nCy+vD8PKGMPR0dDB3WoQQ8kiocJjYncpK/Pf1d3H6m+8wbMokfBR1GDPef5se/EQI6bKs0HAl4J+aTCaDn5+fudNAHw83PPPGaxg+dTL4AgGyf1cgLe4CrskuofBqFrT19eZOkRBCOIa+O6lwmIFDP2eMfHYqRkydjH5DvAAAOq0WNwsKoc7OhTo3D1WlN1F16xZu37yFmooq1N25g7o7d1F/9y5q79yFprbWzHtBCPmzo8JhQYXjQT0dHTBolC9cvAaj7yB39PFwg5PbAAgMPKDqQdp6DXRaLXQ6LXQaLXQ6XcPvWi2YTgedVgfGdNz6TZ7A+MCvesseWs/QkxsfbG+67T/9x4qQLmHn28txq6CwXa819N0peNSkyKO5fbMMKWdikXImVq9dbNcTPXs7wM6xN2zs7CCyEUNkYwORjTVENjYQisXg8fng8XkN/+XxwRPwwePxYMXngc8XwIrHu3/1+kMXsT94VbveFe4PXe2u91trX0NXzBNiMTR1HT8EToXDQt2tuo27VbdRej3f3KkQQogeOquKEEJIm1DhIIQQ0iYmKxzW1tZITEyEQqGASqXC2rVrm6wTEhICtVoNuVwOuVyOxYsXc8s0Gg3XfuLECa7d3d0dCQkJuHr1Kg4cOAChUGiqXSCEEGKAyZ4eZWtrywAwgUDAEhISWEBAgN7ykJAQtm3btmZfW1VV1Wz7wYMH2bx58xgAtn37drZkyZJ2P8WKgoKCgsJwmOUJgNXV1QAAoVAIoVBo8LTOtnj66adx5MgRAMDu3bsxa9asR94mIYSQ1jNp4eDxeJDL5VCr1YiJiUFSUlKTdebMmQOlUonDhw/D1dWVaxeLxZDJZIiPj8fMmTMBAI6OjigvL4dWqwUAFBQUoH///qbcBUIIIc0weXdHIpGw2NhY5uPjo9feu3dvJhKJGAD2xhtvsF9++YVb5uLiwgAwDw8PlpOTwwYNGsSkUinLzMzk1nF1dWUpKSnNvmdoaCiTyWRMJpOxnJwcs3f5KCgoKLpamGWoqlFFRQXi4uIQFBSk137r1i3U1dUBAHbs2IFRo0Zxy4qKigAAOTk5iIuLg6+vL0pLS2Fvbw8+nw8AcHV1RWFh81dE7tixA35+fvDz80NpaakpdosQQrolk10AKJVKUV9fj4qKCojFYkyZMgUbNmzQW8fZ2RnFxcUAgOeeew5XrlwBANjb26OmpgZ1dXVwdHTEuHHjsHHjRgDA2bNn8cILL+DgwYMICQnRO+PKEDc3N8hksnbvR1csPJR35+qKeXfFnAHKuzO5ubkZXGaSLs4TTzzBLl26xJRKJUtNTWWrV69mAFhYWBibMWMGA8A+++wzplKpmEKhYLGxsWzIkCEMABs7dixLSUlhCoWCpaSksEWLFnHb9fDwYImJiSwzM5MdOnSIG+oyVXTVM7Iob8r7z5gz5W0xYfYELDq66j825U15/xlzprwtI+jKcUIIIW3CB7DW3ElYukuXLpk7hXahvDtXV8y7K+YMUN7m1i2ex0EIIaTj0FAVIYSQNqHCQQghpE2ocLRg6tSpSE9PR2ZmJlasWGHudJrl6uqK2NhYXL58GSqVCu+88w4AYM2aNSgoKODuMDxt2jQzZ9pUTk4OUlJSIJfLuetsHBwccObMGVy9ehVnzpyBvb29mbPU5+3tzR1TuVyOiooKvPvuuxZ5vHfu3IkbN24gNTWVa2vp+G7ZsgWZmZlQKpXw9fU1R8oAms9748aNuHLlCpRKJY4dOwaJRAKg4TqDmpoa7rhv377dXGk3m3dLn4t//OMfyMzMRHp6OgIDA82R8iMx+6ldlhg8Ho9lZWUxDw8PJhQKmUKhYEOHDjV7Xg+Hs7Mz8/X1ZQBYz549WUZGBhs6dChbs2YNe//9982eX0uRk5PDHB0d9do2bNjAVqxYwQCwFStWsPXr15s9z5Y+I0VFRWzgwIEWebzHjx/PfH19WWpqqtHjO23aNPbTTz8xACwgIIAlJCRYVN7PPPMM4/P5DABbv349l7ebm5veepZ2vA19LoYOHcoUCgUTiUTM3d2dZWVlMR6PZ/Z9aG1Qj8MAf39/ZGVlIScnB/X19Thw4AB3s0VLUlxcDLlcDgC4ffs2rly50qVv/Dhz5kzs3r0bgOXf/Xjy5Mm4du0a8vLyzJ1Ks86fP49bt27ptRk6vjNnzsSePXsAAImJibC3t4ezs3PnJnxPc3nHxMRwNzdNSEjQuyGqpWgub0NmzpyJAwcOoK6uDrm5ucjKyoK/v7+JM+w4VDgM6N+/P/Lz7z/vuyvcidfNzQ2+vr5ITEwEACxduhRKpRI7d+60uCEfAGCM4cyZM0hOTkZoaCgAoG/fvtxtaIqLi9GnTx9zptii+fPnIyIigvvd0o83YPj4dqXP+6JFixAdHc397uHhgUuXLiEuLg5PPvmkGTNrXnOfi650vJtDhcMAKyurJm0d8TwRU7G1tcXRo0exbNkyVFVVYfv27Rg8eDBGjBiBoqIibNq0ydwpNjFu3DiMGjUK06ZNw1tvvYXx48ebO6VWEwqFeO6553D48GEA6BLHuyVd5fO+cuVKaDQa7Nu3D0DDzVAHDhyIkSNH4u9//zv2798POzs7M2d5n6HPRVc53oZQ4TCgoKAAAwYM4H5v6U685iYQCHD06FHs27cPx48fBwCo1WrodDowxrBjxw6L7AY33gG5pKQEx48fh7+/P27cuMENkTg7O0OtVpszRYOmTZuGS5cucfl1heMNwODx7Qqf9wULFmD69Ol4+eWXuba6ujpueOjSpUu4du0avL29zZViE4Y+F13heLeECocBMpkMXl5ecHd3h1AoxPz583Hy5Elzp9WsnTt34sqVK/jyyy+5tgfHp2fPng2VSmWO1Azq0aMHevbsyf0cGBgIlUqFkydPIiQkBABaffdjcwgODtYbprL0493I0PE9efIkFixYAAAICAhARUUFN6RlCaZOnYoVK1bgueeew507d7h2qVQKHq/ha8zDwwNeXl7Izs42V5pNGPpcnDx5EvPnz4dIJIK7uzu8vLyafdCdJTP7DL2lxrRp01hGRgbLyspiK1euNHs+zcW4ceMYY4wplUoml8uZXC5n06ZNY3v27GEpKSlMqVSyEydOMGdnZ7Pn+mB4eHgwhULBFAoFU6lU3PHt3bs3+/nnn9nVq1fZzz//zBwcHMye68NhY2PDSktLWa9evbg2Szze+/fvZ4WFhayuro7l5+ezRYsWtXh8v/rqK5aVlcVSUlLYqFGjLCrvzMxMlpeXx33Gt2/fzgCw559/nrvD9u+//86mT59uUXm39LlYuXIly8rKYunp6SwoKMjsn5e2BN1yhBBCSJvQUBUhhJA2ocJBCCGkTahwEEIIaRMqHIQQQtqECgchhJA2ocJBiBG//fYbgIZbugQHB3fotj/66KNm34sQS0an4xLSShMnTsQHH3yAGTNmtPo1PB4POp3O4PKqqiqLukUGIa1BPQ5CjKiqqgIArF+/HuPHj4dcLseyZcvA4/GwceNGJCUlQalU4vXXXwfQUGBiY2Oxb98+7tkMx48fR3JyMlQqFXdDx3//+9+wsbGBXC7H//73P733AhqeQZGamoqUlBTMnTuX2/bZs2dx+PBhXLlyhXtd4/bS0tKgVCrx+eefm/7AkG7N7FchUlBYclRVVTEAbOLEiSwyMpJrDw0NZR9//DEDwEQiEZPJZMzd3Z1NnDiR3b59m7m7u3PrNl6hLRaLWWpqKuvdu7feth9+r+eff56dOXOG8Xg81qdPH3b9+nXm7OzMJk6cyMrLy1n//v2ZlZUVu3jxIhs3bhxzcHBg6enp3HYkEonZjxvFnzeox0FIOwUGBmLBggWQy+VITEyEo6MjvLy8AABJSUnIzc3l1n3nnXegUCiQkJCAAQMGcOsZ8uSTTyIiIgI6nQ5qtRq//vor/Pz8uG3/8ccfYIxBoVDA3d0dlZWVuHv3Lr777jvMnj0bNTU1JttvQqhwENJOVlZWePvtt+Hr6wtfX18MGjQIMTExAIDq6mpuvYkTJ2LKlCkYO3YsRowYAblcDrFYbHTbhtTW1nI/a7VaCAQCaLVa+Pv74+jRo5g1axZOnTr1iHtHiGFUOAhppYcnsk+fPo0333wTAoEAAODl5YUePXo0eZ1EIkFZWRnu3LmDIUOGYMyYMdyy+vp67vUPOnfuHObNmwcejwepVIoJEya0ePdUW1tbSCQSREdHY9myZRgxYsSj7CohLWr6iSWENCslJQUajQYKhQK7du3Cli1b4O7ujkuXLsHKygolJSXNPur21KlTWLJkCZRKJTIyMpCQkMAt++9//4uUlBRcunQJr7zyCtd+/PhxjB07FkqlEowxfPjhh7hx4wYee+yxZnOzs7PDiRMnIBaLYWVlhffee6/jDwAh99DpuIQQQtqEhqoIIYS0CRUOQgghbUKFgxBCSJtQ4SCEENImVDgIIYS0CRUOQgghbUKFgxBCSJv8f1WI32UTIYWcAAAAAElFTkSuQmCC",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from matplotlib import style\n",
    "style.use('dark_background')\n",
    "plt.figure()\n",
    "plt.plot(np.arange(0, len(returned_MSE_list)), returned_MSE_list) ## fix\n",
    "plt.title(\"Training Loss: Mean Squared Error \")\n",
    "plt.xlabel(\"iterations\")\n",
    "plt.ylabel(\"Mean Squared Error\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>bedrooms</th>\n",
       "      <th>bathrooms</th>\n",
       "      <th>sqft_living</th>\n",
       "      <th>sqft_lot</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>8</td>\n",
       "      <td>25</td>\n",
       "      <td>50000</td>\n",
       "      <td>225000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   bedrooms  bathrooms  sqft_living  sqft_lot\n",
       "0         8         25        50000    225000"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fancy_house[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Norm of Gradient:  5185991300.341278\n",
      "Iteration: 2, Norm of Gradient:  2072809273.682034\n",
      "Iteration: 3, Norm of Gradient:  1003031617.6659364\n",
      "Iteration: 4, Norm of Gradient:  642460381.35682\n",
      "Iteration: 5, Norm of Gradient:  491564663.9651663\n",
      "Iteration: 6, Norm of Gradient:  401327662.5138394\n",
      "Iteration: 7, Norm of Gradient:  335293725.70789415\n",
      "Iteration: 8, Norm of Gradient:  283150637.32664466\n",
      "Iteration: 9, Norm of Gradient:  240612306.80368227\n",
      "Iteration: 10, Norm of Gradient:  205271567.18063077\n",
      "Iteration: 11, Norm of Gradient:  175566360.17691374\n",
      "Iteration: 12, Norm of Gradient:  150405267.71778882\n",
      "Iteration: 13, Norm of Gradient:  128985141.0977269\n",
      "Iteration: 14, Norm of Gradient:  110689659.73710419\n",
      "Iteration: 15, Norm of Gradient:  95029787.73691219\n",
      "Iteration: 16, Norm of Gradient:  81607563.5331986\n",
      "Iteration: 17, Norm of Gradient:  70093224.45180923\n",
      "Iteration: 18, Norm of Gradient:  60210092.023285896\n",
      "Iteration: 19, Norm of Gradient:  51724080.726320885\n",
      "Iteration: 20, Norm of Gradient:  44436051.73710533\n",
      "Iteration: 21, Norm of Gradient:  38175991.63731683\n",
      "Iteration: 22, Norm of Gradient:  32798419.68866373\n",
      "Iteration: 23, Norm of Gradient:  28178665.041517433\n",
      "Iteration: 24, Norm of Gradient:  24209789.973726846\n",
      "Iteration: 25, Norm of Gradient:  20800012.837582678\n",
      "Iteration: 26, Norm of Gradient:  17870530.14911028\n",
      "Iteration: 27, Norm of Gradient:  15353665.142061964\n",
      "Iteration: 28, Norm of Gradient:  13191287.83153912\n",
      "Iteration: 29, Norm of Gradient:  11333463.454726696\n",
      "Iteration: 30, Norm of Gradient:  9737294.456979318\n",
      "Iteration: 31, Norm of Gradient:  8365927.310744024\n",
      "Iteration: 32, Norm of Gradient:  7187700.16011523\n",
      "Iteration: 33, Norm of Gradient:  6175411.025584337\n",
      "Iteration: 34, Norm of Gradient:  5305689.354199284\n",
      "Iteration: 35, Norm of Gradient:  4558456.231989985\n",
      "Iteration: 36, Norm of Gradient:  3916460.701827995\n",
      "Iteration: 37, Norm of Gradient:  3364881.430185165\n",
      "Iteration: 38, Norm of Gradient:  2890984.498506479\n",
      "Iteration: 39, Norm of Gradient:  2483829.403503948\n",
      "Iteration: 40, Norm of Gradient:  2134016.4706301177\n",
      "Iteration: 41, Norm of Gradient:  1833469.8448890634\n",
      "Iteration: 42, Norm of Gradient:  1575251.0465863126\n",
      "Iteration: 43, Norm of Gradient:  1353398.7863580682\n",
      "Iteration: 44, Norm of Gradient:  1162791.3406875085\n",
      "Iteration: 45, Norm of Gradient:  999028.3102748137\n",
      "Iteration: 46, Norm of Gradient:  858329.0312961248\n",
      "Iteration: 47, Norm of Gradient:  737445.2941307443\n",
      "Iteration: 48, Norm of Gradient:  633586.3545070888\n",
      "Iteration: 49, Norm of Gradient:  544354.5058127317\n",
      "Iteration: 50, Norm of Gradient:  467689.72517467075\n",
      "Iteration: 51, Norm of Gradient:  401822.1153782823\n",
      "Iteration: 52, Norm of Gradient:  345231.04468508315\n",
      "Iteration: 53, Norm of Gradient:  296610.04124565993\n",
      "Iteration: 54, Norm of Gradient:  254836.631650061\n",
      "Iteration: 55, Norm of Gradient:  218946.4272968999\n",
      "Iteration: 56, Norm of Gradient:  188110.86034126085\n",
      "Iteration: 57, Norm of Gradient:  161618.05522591327\n",
      "Iteration: 58, Norm of Gradient:  138856.39419067997\n",
      "Iteration: 59, Norm of Gradient:  119300.39735192014\n",
      "Iteration: 60, Norm of Gradient:  102498.59137712263\n",
      "Iteration: 61, Norm of Gradient:  88063.08669160641\n",
      "Iteration: 62, Norm of Gradient:  75660.62258468485\n",
      "Iteration: 63, Norm of Gradient:  65004.8734941616\n",
      "Iteration: 64, Norm of Gradient:  55849.83884147219\n",
      "Iteration: 65, Norm of Gradient:  47984.16381647592\n",
      "Iteration: 66, Norm of Gradient:  41226.26000227245\n",
      "Iteration: 67, Norm of Gradient:  35420.113191379816\n",
      "Iteration: 68, Norm of Gradient:  30431.681613739554\n",
      "Iteration: 69, Norm of Gradient:  26145.80142118741\n",
      "Iteration: 70, Norm of Gradient:  22463.527998524518\n",
      "Iteration: 71, Norm of Gradient:  19299.851704589397\n",
      "Iteration: 72, Norm of Gradient:  16581.735328996216\n",
      "Iteration: 73, Norm of Gradient:  14246.427937840457\n",
      "Iteration: 74, Norm of Gradient:  12240.016195559121\n",
      "Iteration: 75, Norm of Gradient:  10516.179714132737\n",
      "Iteration: 76, Norm of Gradient:  9035.121686815824\n",
      "Iteration: 77, Norm of Gradient:  7762.650136959562\n",
      "Iteration: 78, Norm of Gradient:  6669.388552388699\n",
      "Iteration: 79, Norm of Gradient:  5730.097696249386\n",
      "Iteration: 80, Norm of Gradient:  4923.092925836472\n",
      "Iteration: 81, Norm of Gradient:  4229.7435830385675\n",
      "Iteration: 82, Norm of Gradient:  3634.0428762610636\n",
      "Iteration: 83, Norm of Gradient:  3122.2383502397174\n",
      "Iteration: 84, Norm of Gradient:  2682.514392035774\n",
      "Iteration: 85, Norm of Gradient:  2304.719452948032\n",
      "Iteration: 86, Norm of Gradient:  1980.131689900992\n",
      "Iteration: 87, Norm of Gradient:  1701.2576109548975\n",
      "Iteration: 88, Norm of Gradient:  1461.6590771226397\n",
      "Iteration: 89, Norm of Gradient:  1255.8046727823614\n",
      "Iteration: 90, Norm of Gradient:  1078.9420059202507\n",
      "Iteration: 91, Norm of Gradient:  926.9879918894652\n",
      "Iteration: 92, Norm of Gradient:  796.434592967501\n",
      "Iteration: 93, Norm of Gradient:  684.2678293155387\n",
      "Iteration: 94, Norm of Gradient:  587.8981977521337\n",
      "Iteration: 95, Norm of Gradient:  505.10089251114147\n",
      "Iteration: 96, Norm of Gradient:  433.96443834413793\n",
      "Iteration: 97, Norm of Gradient:  372.84656755031074\n",
      "Iteration: 98, Norm of Gradient:  320.3363007887369\n",
      "Iteration: 99, Norm of Gradient:  275.2213765680308\n",
      "Iteration: 100, Norm of Gradient:  236.46026396004248\n",
      "Iteration: 101, Norm of Gradient:  203.15811609779223\n",
      "Iteration: 102, Norm of Gradient:  174.54611362500927\n",
      "Iteration: 103, Norm of Gradient:  149.9637149084198\n",
      "Iteration: 104, Norm of Gradient:  128.84340596506934\n",
      "Iteration: 105, Norm of Gradient:  110.69759942094987\n",
      "Iteration: 106, Norm of Gradient:  95.10737820286772\n",
      "Iteration: 107, Norm of Gradient:  81.71282364137818\n",
      "Iteration: 108, Norm of Gradient:  70.20470596091752\n",
      "Iteration: 109, Norm of Gradient:  60.31734686124238\n",
      "Iteration: 110, Norm of Gradient:  51.82248562717168\n",
      "Iteration: 111, Norm of Gradient:  44.52400736954146\n",
      "Iteration: 112, Norm of Gradient:  38.25341861630821\n",
      "Iteration: 113, Norm of Gradient:  32.86595534619491\n",
      "Iteration: 114, Norm of Gradient:  28.2372414184941\n",
      "Iteration: 115, Norm of Gradient:  24.260417481847664\n",
      "Iteration: 116, Norm of Gradient:  20.843674286707845\n",
      "Iteration: 117, Norm of Gradient:  17.90813225525579\n",
      "Iteration: 118, Norm of Gradient:  15.38602018342459\n",
      "Iteration: 119, Norm of Gradient:  13.219112554092694\n",
      "Iteration: 120, Norm of Gradient:  11.357384030659286\n",
      "Iteration: 121, Norm of Gradient:  9.757853936664233\n",
      "Iteration: 122, Norm of Gradient:  8.383596087119205\n",
      "Iteration: 123, Norm of Gradient:  7.202882073294902\n",
      "Iteration: 124, Norm of Gradient:  6.1884564264712\n",
      "Iteration: 125, Norm of Gradient:  5.316897839692662\n",
      "Iteration: 126, Norm of Gradient:  4.568086645201592\n",
      "Iteration: 127, Norm of Gradient:  3.924735093014931\n",
      "Iteration: 128, Norm of Gradient:  3.3719910714128307\n",
      "Iteration: 129, Norm of Gradient:  2.8970929123692004\n",
      "Iteration: 130, Norm of Gradient:  2.4890773531248205\n",
      "Iteration: 131, Norm of Gradient:  2.138525744185645\n",
      "Iteration: 132, Norm of Gradient:  1.837343578995839\n",
      "Iteration: 133, Norm of Gradient:  1.5785793673753787\n",
      "Iteration: 134, Norm of Gradient:  1.3562582192798927\n",
      "Iteration: 135, Norm of Gradient:  1.1652481243501969\n",
      "Iteration: 136, Norm of Gradient:  1.0011390689593778\n",
      "Iteration: 137, Norm of Gradient:  0.860142720426832\n",
      "Iteration: 138, Norm of Gradient:  0.7390035070014153\n",
      "Iteration: 139, Norm of Gradient:  0.634924984948741\n",
      "Iteration: 140, Norm of Gradient:  0.5455048330894973\n",
      "Iteration: 141, Norm of Gradient:  0.4686777894529069\n",
      "Iteration: 142, Norm of Gradient:  0.4026710536850832\n",
      "Iteration: 143, Norm of Gradient:  0.345960585052866\n",
      "Iteration: 144, Norm of Gradient:  0.29723698569834417\n",
      "Iteration: 145, Norm of Gradient:  0.2553752068072756\n",
      "Iteration: 146, Norm of Gradient:  0.21940940166470793\n",
      "Iteration: 147, Norm of Gradient:  0.18850859410886217\n",
      "Iteration: 148, Norm of Gradient:  0.16195957464729308\n",
      "Iteration: 149, Norm of Gradient:  0.1391499814572064\n",
      "Iteration: 150, Norm of Gradient:  0.11955250150566363\n",
      "Iteration: 151, Norm of Gradient:  0.1027154060545937\n",
      "Iteration: 152, Norm of Gradient:  0.08824941802871114\n",
      "Iteration: 153, Norm of Gradient:  0.0758206841867012\n",
      "Iteration: 154, Norm of Gradient:  0.06514228416435922\n",
      "Iteration: 155, Norm of Gradient:  0.05596770814107544\n",
      "Iteration: 156, Norm of Gradient:  0.048085806424481695\n",
      "Iteration: 157, Norm of Gradient:  0.04131359540742661\n",
      "Iteration: 158, Norm of Gradient:  0.03549512751606251\n",
      "Iteration: 159, Norm of Gradient:  0.030495728700220437\n",
      "Iteration: 160, Norm of Gradient:  0.026200891067624667\n",
      "Iteration: 161, Norm of Gradient:  0.022511268719865578\n",
      "Iteration: 162, Norm of Gradient:  0.019340891727169754\n",
      "Iteration: 163, Norm of Gradient:  0.016617064731535645\n",
      "Iteration: 164, Norm of Gradient:  0.014276550440661314\n",
      "Iteration: 165, Norm of Gradient:  0.012265978565135812\n",
      "Iteration: 166, Norm of Gradient:  0.01053841040708231\n",
      "Iteration: 167, Norm of Gradient:  0.009054352809669512\n",
      "R-Squared:   0.51011\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([[14898622.65167107]])"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Repeat b)\n",
    "\n",
    "beta,returned_MSE_list, y, y_pred, standardizer = gd(train_data[features],y = y_train, learning_parameter = .00004, tau = .01)\n",
    "\n",
    "#need to standardize fancy_house data using the mean and std from the training data\n",
    "fancy_house_scaled = standardizer.transform(fancy_house[features])\n",
    "\n",
    "\n",
    "fancy_house_scaled @ beta\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration: 1, Norm of Gradient:  5929348665.552127\n",
      "Iteration: 2, Norm of Gradient:  5434064126.398718\n",
      "Iteration: 3, Norm of Gradient:  5001165971.070366\n",
      "Iteration: 4, Norm of Gradient:  4612841411.500545\n",
      "Iteration: 5, Norm of Gradient:  4259989218.1344166\n",
      "Iteration: 6, Norm of Gradient:  3937087069.9884963\n",
      "Iteration: 7, Norm of Gradient:  3640378129.26082\n",
      "Iteration: 8, Norm of Gradient:  3367070636.0019455\n",
      "Iteration: 9, Norm of Gradient:  3114941531.5564575\n",
      "Iteration: 10, Norm of Gradient:  2882128741.941849\n",
      "Iteration: 11, Norm of Gradient:  2667017880.588795\n",
      "Iteration: 12, Norm of Gradient:  2468177711.9820256\n",
      "Iteration: 13, Norm of Gradient:  2284321347.1613126\n",
      "Iteration: 14, Norm of Gradient:  2114281348.4646208\n",
      "Iteration: 15, Norm of Gradient:  1956992597.2735624\n",
      "Iteration: 16, Norm of Gradient:  1811479682.7627122\n",
      "Iteration: 17, Norm of Gradient:  1676847067.1053188\n",
      "Iteration: 18, Norm of Gradient:  1552271062.0828736\n",
      "Iteration: 19, Norm of Gradient:  1436993063.3333688\n",
      "Iteration: 20, Norm of Gradient:  1330313709.6859696\n",
      "Iteration: 21, Norm of Gradient:  1231587757.1783047\n",
      "Iteration: 22, Norm of Gradient:  1140219527.1457355\n",
      "Iteration: 23, Norm of Gradient:  1055658829.3482808\n",
      "Iteration: 24, Norm of Gradient:  977397287.0631875\n",
      "Iteration: 25, Norm of Gradient:  904965008.0973912\n",
      "Iteration: 26, Norm of Gradient:  837927557.375925\n",
      "Iteration: 27, Norm of Gradient:  775883195.1470059\n",
      "Iteration: 28, Norm of Gradient:  718460351.0662048\n",
      "Iteration: 29, Norm of Gradient:  665315309.1718067\n",
      "Iteration: 30, Norm of Gradient:  616130082.4731027\n",
      "Iteration: 31, Norm of Gradient:  570610458.8250483\n",
      "Iteration: 32, Norm of Gradient:  528484202.14821494\n",
      "Iteration: 33, Norm of Gradient:  489499395.0070665\n",
      "Iteration: 34, Norm of Gradient:  453422910.18008405\n",
      "Iteration: 35, Norm of Gradient:  420039000.2139799\n",
      "Iteration: 36, Norm of Gradient:  389147995.1056968\n",
      "Iteration: 37, Norm of Gradient:  360565099.24107116\n",
      "Iteration: 38, Norm of Gradient:  334119279.5699056\n",
      "Iteration: 39, Norm of Gradient:  309652237.73817813\n",
      "Iteration: 40, Norm of Gradient:  287017459.54859763\n",
      "Iteration: 41, Norm of Gradient:  266079335.6957872\n",
      "Iteration: 42, Norm of Gradient:  246712348.2340911\n",
      "Iteration: 43, Norm of Gradient:  228800317.69408733\n",
      "Iteration: 44, Norm of Gradient:  212235706.1760762\n",
      "Iteration: 45, Norm of Gradient:  196918972.12158102\n",
      "Iteration: 46, Norm of Gradient:  182757972.80258596\n",
      "Iteration: 47, Norm of Gradient:  169667410.87731966\n",
      "Iteration: 48, Norm of Gradient:  157568321.64468908\n",
      "Iteration: 49, Norm of Gradient:  146387597.89052165\n",
      "Iteration: 50, Norm of Gradient:  136057549.4603363\n",
      "Iteration: 51, Norm of Gradient:  126515494.91838658\n",
      "Iteration: 52, Norm of Gradient:  117703382.86347146\n",
      "Iteration: 53, Norm of Gradient:  109567440.67076387\n",
      "Iteration: 54, Norm of Gradient:  102057848.61767733\n",
      "Iteration: 55, Norm of Gradient:  95128437.53209636\n",
      "Iteration: 56, Norm of Gradient:  88736408.27453643\n",
      "Iteration: 57, Norm of Gradient:  82842071.53266263\n",
      "Iteration: 58, Norm of Gradient:  77408606.56720872\n",
      "Iteration: 59, Norm of Gradient:  72401837.7020458\n",
      "Iteration: 60, Norm of Gradient:  67790027.49645685\n",
      "Iteration: 61, Norm of Gradient:  63543685.6718772\n",
      "Iteration: 62, Norm of Gradient:  59635392.98516934\n",
      "Iteration: 63, Norm of Gradient:  56039639.34158613\n",
      "Iteration: 64, Norm of Gradient:  52732675.51830696\n",
      "Iteration: 65, Norm of Gradient:  49692377.91946028\n",
      "Iteration: 66, Norm of Gradient:  46898125.802349225\n",
      "Iteration: 67, Norm of Gradient:  44330690.40057895\n",
      "Iteration: 68, Norm of Gradient:  41972135.32396021\n",
      "Iteration: 69, Norm of Gradient:  39805727.54178413\n",
      "Iteration: 70, Norm of Gradient:  37815858.163246766\n",
      "Iteration: 71, Norm of Gradient:  35987972.12748085\n",
      "Iteration: 72, Norm of Gradient:  34308505.8192515\n",
      "Iteration: 73, Norm of Gradient:  32764831.548876207\n",
      "Iteration: 74, Norm of Gradient:  31345207.789160464\n",
      "Iteration: 75, Norm of Gradient:  30038734.057865977\n",
      "Iteration: 76, Norm of Gradient:  28835309.376512717\n",
      "Iteration: 77, Norm of Gradient:  27725593.324668452\n",
      "Iteration: 78, Norm of Gradient:  26700968.83755226\n",
      "Iteration: 79, Norm of Gradient:  25753506.053320564\n",
      "Iteration: 80, Norm of Gradient:  24875926.691715088\n",
      "Iteration: 81, Norm of Gradient:  24061568.62358942\n",
      "Iteration: 82, Norm of Gradient:  23304350.458197802\n",
      "Iteration: 83, Norm of Gradient:  22598736.121233374\n",
      "Iteration: 84, Norm of Gradient:  21939699.514364537\n",
      "Iteration: 85, Norm of Gradient:  21322689.43281021\n",
      "Iteration: 86, Norm of Gradient:  20743594.97134941\n",
      "Iteration: 87, Norm of Gradient:  20198711.673657946\n",
      "Iteration: 88, Norm of Gradient:  19684708.679682262\n",
      "Iteration: 89, Norm of Gradient:  19198597.106378518\n",
      "Iteration: 90, Norm of Gradient:  18737699.86440707\n",
      "Iteration: 91, Norm of Gradient:  18299623.072631873\n",
      "Iteration: 92, Norm of Gradient:  17882229.18816626\n",
      "Iteration: 93, Norm of Gradient:  17483611.925887767\n",
      "Iteration: 94, Norm of Gradient:  17102073.000454847\n",
      "Iteration: 95, Norm of Gradient:  16736100.687655795\n",
      "Iteration: 96, Norm of Gradient:  16384350.1712988\n",
      "Iteration: 97, Norm of Gradient:  16045625.617164697\n",
      "Iteration: 98, Norm of Gradient:  15718863.89665839\n",
      "Iteration: 99, Norm of Gradient:  15403119.86920343\n",
      "Iteration: 100, Norm of Gradient:  15097553.123602416\n",
      "Iteration: 101, Norm of Gradient:  14801416.073723737\n",
      "Iteration: 102, Norm of Gradient:  14514043.302331002\n",
      "Iteration: 103, Norm of Gradient:  14234842.047920343\n",
      "Iteration: 104, Norm of Gradient:  13963283.732447382\n",
      "Iteration: 105, Norm of Gradient:  13698896.432324558\n",
      "Iteration: 106, Norm of Gradient:  13441258.200506955\n",
      "Iteration: 107, Norm of Gradient:  13189991.153581275\n",
      "Iteration: 108, Norm of Gradient:  12944756.244139798\n",
      "Iteration: 109, Norm of Gradient:  12705248.64523237\n",
      "Iteration: 110, Norm of Gradient:  12471193.680029709\n",
      "Iteration: 111, Norm of Gradient:  12242343.236033812\n",
      "Iteration: 112, Norm of Gradient:  12018472.60902194\n",
      "Iteration: 113, Norm of Gradient:  11799377.727413807\n",
      "Iteration: 114, Norm of Gradient:  11584872.71287059\n",
      "Iteration: 115, Norm of Gradient:  11374787.7376387\n",
      "Iteration: 116, Norm of Gradient:  11168967.143463839\n",
      "Iteration: 117, Norm of Gradient:  10967267.790811546\n",
      "Iteration: 118, Norm of Gradient:  10769557.610654538\n",
      "Iteration: 119, Norm of Gradient:  10575714.334295675\n",
      "Iteration: 120, Norm of Gradient:  10385624.379531909\n",
      "Iteration: 121, Norm of Gradient:  10199181.87401391\n",
      "Iteration: 122, Norm of Gradient:  10016287.798939971\n",
      "Iteration: 123, Norm of Gradient:  9836849.238231244\n",
      "Iteration: 124, Norm of Gradient:  9660778.720135551\n",
      "Iteration: 125, Norm of Gradient:  9487993.639778733\n",
      "Iteration: 126, Norm of Gradient:  9318415.752601838\n",
      "Iteration: 127, Norm of Gradient:  9151970.729839774\n",
      "Iteration: 128, Norm of Gradient:  8988587.768296938\n",
      "Iteration: 129, Norm of Gradient:  8828199.247630347\n",
      "Iteration: 130, Norm of Gradient:  8670740.429193491\n",
      "Iteration: 131, Norm of Gradient:  8516149.191234933\n",
      "Iteration: 132, Norm of Gradient:  8364365.795893693\n",
      "Iteration: 133, Norm of Gradient:  8215332.684005937\n",
      "Iteration: 134, Norm of Gradient:  8068994.294237709\n",
      "Iteration: 135, Norm of Gradient:  7925296.903486612\n",
      "Iteration: 136, Norm of Gradient:  7784188.485899345\n",
      "Iteration: 137, Norm of Gradient:  7645618.588166678\n",
      "Iteration: 138, Norm of Gradient:  7509538.219052804\n",
      "Iteration: 139, Norm of Gradient:  7375899.75139984\n",
      "Iteration: 140, Norm of Gradient:  7244656.835027814\n",
      "Iteration: 141, Norm of Gradient:  7115764.319195134\n",
      "Iteration: 142, Norm of Gradient:  6989178.183409592\n",
      "Iteration: 143, Norm of Gradient:  6864855.475574035\n",
      "Iteration: 144, Norm of Gradient:  6742754.256548517\n",
      "Iteration: 145, Norm of Gradient:  6622833.5503421575\n",
      "Iteration: 146, Norm of Gradient:  6505053.299238138\n",
      "Iteration: 147, Norm of Gradient:  6389374.323260849\n",
      "Iteration: 148, Norm of Gradient:  6275758.283441257\n",
      "Iteration: 149, Norm of Gradient:  6164167.64843653\n",
      "Iteration: 150, Norm of Gradient:  6054565.664083486\n",
      "Iteration: 151, Norm of Gradient:  5946916.325553377\n",
      "Iteration: 152, Norm of Gradient:  5841184.35178285\n",
      "Iteration: 153, Norm of Gradient:  5737335.161928477\n",
      "Iteration: 154, Norm of Gradient:  5635334.853591909\n",
      "Iteration: 155, Norm of Gradient:  5535150.182634543\n",
      "Iteration: 156, Norm of Gradient:  5436748.5443722205\n",
      "Iteration: 157, Norm of Gradient:  5340097.956024239\n",
      "Iteration: 158, Norm of Gradient:  5245167.04025142\n",
      "Iteration: 159, Norm of Gradient:  5151925.009685267\n",
      "Iteration: 160, Norm of Gradient:  5060341.652325782\n",
      "Iteration: 161, Norm of Gradient:  4970387.3177304035\n",
      "Iteration: 162, Norm of Gradient:  4882032.903897181\n",
      "Iteration: 163, Norm of Gradient:  4795249.844790145\n",
      "Iteration: 164, Norm of Gradient:  4710010.09842668\n",
      "Iteration: 165, Norm of Gradient:  4626286.135485696\n",
      "Iteration: 166, Norm of Gradient:  4544050.928379358\n",
      "Iteration: 167, Norm of Gradient:  4463277.940754465\n",
      "Iteration: 168, Norm of Gradient:  4383941.11737883\n",
      "Iteration: 169, Norm of Gradient:  4306014.874386888\n",
      "Iteration: 170, Norm of Gradient:  4229474.089847947\n",
      "Iteration: 171, Norm of Gradient:  4154294.094638883\n",
      "Iteration: 172, Norm of Gradient:  4080450.6635982934\n",
      "Iteration: 173, Norm of Gradient:  4007920.006934663\n",
      "Iteration: 174, Norm of Gradient:  3936678.76187838\n",
      "Iteration: 175, Norm of Gradient:  3866703.9845646503\n",
      "Iteration: 176, Norm of Gradient:  3797973.142117676\n",
      "Iteration: 177, Norm of Gradient:  3730464.10494665\n",
      "Iteration: 178, Norm of Gradient:  3664155.1392208184\n",
      "Iteration: 179, Norm of Gradient:  3599024.899530708\n",
      "Iteration: 180, Norm of Gradient:  3535052.421712407\n",
      "Iteration: 181, Norm of Gradient:  3472217.1158377603\n",
      "Iteration: 182, Norm of Gradient:  3410498.759358711\n",
      "Iteration: 183, Norm of Gradient:  3349877.4903974985\n",
      "Iteration: 184, Norm of Gradient:  3290333.801182581\n",
      "Iteration: 185, Norm of Gradient:  3231848.5316164466\n",
      "Iteration: 186, Norm of Gradient:  3174402.862977124\n",
      "Iteration: 187, Norm of Gradient:  3117978.311750064\n",
      "Iteration: 188, Norm of Gradient:  3062556.7235760447\n",
      "Iteration: 189, Norm of Gradient:  3008120.2673242507\n",
      "Iteration: 190, Norm of Gradient:  2954651.429278966\n",
      "Iteration: 191, Norm of Gradient:  2902133.0074335635\n",
      "Iteration: 192, Norm of Gradient:  2850548.1059023133\n",
      "Iteration: 193, Norm of Gradient:  2799880.1294315634\n",
      "Iteration: 194, Norm of Gradient:  2750112.7780174683\n",
      "Iteration: 195, Norm of Gradient:  2701230.0416209116\n",
      "Iteration: 196, Norm of Gradient:  2653216.194985645\n",
      "Iteration: 197, Norm of Gradient:  2606055.7925490555\n",
      "Iteration: 198, Norm of Gradient:  2559733.6634471132\n",
      "Iteration: 199, Norm of Gradient:  2514234.9066126077\n",
      "Iteration: 200, Norm of Gradient:  2469544.8859614525\n",
      "Iteration: 201, Norm of Gradient:  2425649.2256656834\n",
      "Iteration: 202, Norm of Gradient:  2382533.8055194495\n",
      "Iteration: 203, Norm of Gradient:  2340184.756376329\n",
      "Iteration: 204, Norm of Gradient:  2298588.455682951\n",
      "Iteration: 205, Norm of Gradient:  2257731.523083394\n",
      "Iteration: 206, Norm of Gradient:  2217600.8161080712\n",
      "Iteration: 207, Norm of Gradient:  2178183.425937358\n",
      "Iteration: 208, Norm of Gradient:  2139466.6732446644\n",
      "Iteration: 209, Norm of Gradient:  2101438.1041092533\n",
      "Iteration: 210, Norm of Gradient:  2064085.4860077244\n",
      "Iteration: 211, Norm of Gradient:  2027396.8038736007\n",
      "Iteration: 212, Norm of Gradient:  1991360.2562289159\n",
      "Iteration: 213, Norm of Gradient:  1955964.2513851759\n",
      "Iteration: 214, Norm of Gradient:  1921197.4037110878\n",
      "Iteration: 215, Norm of Gradient:  1887048.5299674836\n",
      "Iteration: 216, Norm of Gradient:  1853506.6457095658\n",
      "Iteration: 217, Norm of Gradient:  1820560.961748802\n",
      "Iteration: 218, Norm of Gradient:  1788200.8806841834\n",
      "Iteration: 219, Norm of Gradient:  1756415.993488888\n",
      "Iteration: 220, Norm of Gradient:  1725196.0761626514\n",
      "Iteration: 221, Norm of Gradient:  1694531.0864420398\n",
      "Iteration: 222, Norm of Gradient:  1664411.16056702\n",
      "Iteration: 223, Norm of Gradient:  1634826.610110709\n",
      "Iteration: 224, Norm of Gradient:  1605767.9188588678\n",
      "Iteration: 225, Norm of Gradient:  1577225.7397517757\n",
      "Iteration: 226, Norm of Gradient:  1549190.8918735576\n",
      "Iteration: 227, Norm of Gradient:  1521654.3575001399\n",
      "Iteration: 228, Norm of Gradient:  1494607.2791992635\n",
      "Iteration: 229, Norm of Gradient:  1468040.9569788151\n",
      "Iteration: 230, Norm of Gradient:  1441946.8454899364\n",
      "Iteration: 231, Norm of Gradient:  1416316.5512770307\n",
      "Iteration: 232, Norm of Gradient:  1391141.8300778158\n",
      "Iteration: 233, Norm of Gradient:  1366414.5841707138\n",
      "Iteration: 234, Norm of Gradient:  1342126.8597720847\n",
      "Iteration: 235, Norm of Gradient:  1318270.8444747867\n",
      "Iteration: 236, Norm of Gradient:  1294838.8647367288\n",
      "Iteration: 237, Norm of Gradient:  1271823.3834119635\n",
      "Iteration: 238, Norm of Gradient:  1249216.9973268702\n",
      "Iteration: 239, Norm of Gradient:  1227012.4348978011\n",
      "Iteration: 240, Norm of Gradient:  1205202.5537928238\n",
      "Iteration: 241, Norm of Gradient:  1183780.3386336828\n",
      "Iteration: 242, Norm of Gradient:  1162738.8987385812\n",
      "Iteration: 243, Norm of Gradient:  1142071.4659077204\n",
      "Iteration: 244, Norm of Gradient:  1121771.3922442382\n",
      "Iteration: 245, Norm of Gradient:  1101832.1480164835\n",
      "Iteration: 246, Norm of Gradient:  1082247.3195577674\n",
      "Iteration: 247, Norm of Gradient:  1063010.6072025534\n",
      "Iteration: 248, Norm of Gradient:  1044115.8232618509\n",
      "Iteration: 249, Norm of Gradient:  1025556.890030289\n",
      "Iteration: 250, Norm of Gradient:  1007327.8378342887\n",
      "Iteration: 251, Norm of Gradient:  989422.8031077638\n",
      "Iteration: 252, Norm of Gradient:  971836.0265089357\n",
      "Iteration: 253, Norm of Gradient:  954561.8510697572\n",
      "Iteration: 254, Norm of Gradient:  937594.7203704041\n",
      "Iteration: 255, Norm of Gradient:  920929.176756198\n",
      "Iteration: 256, Norm of Gradient:  904559.8595823278\n",
      "Iteration: 257, Norm of Gradient:  888481.5034868672\n",
      "Iteration: 258, Norm of Gradient:  872688.9366999475\n",
      "Iteration: 259, Norm of Gradient:  857177.0793766993\n",
      "Iteration: 260, Norm of Gradient:  841940.9419684355\n",
      "Iteration: 261, Norm of Gradient:  826975.6236119254\n",
      "Iteration: 262, Norm of Gradient:  812276.3105575703\n",
      "Iteration: 263, Norm of Gradient:  797838.2746173616\n",
      "Iteration: 264, Norm of Gradient:  783656.8716482632\n",
      "Iteration: 265, Norm of Gradient:  769727.5400538805\n",
      "Iteration: 266, Norm of Gradient:  756045.7993194438\n",
      "Iteration: 267, Norm of Gradient:  742607.2485718646\n",
      "Iteration: 268, Norm of Gradient:  729407.5651608931\n",
      "Iteration: 269, Norm of Gradient:  716442.5032712934\n",
      "Iteration: 270, Norm of Gradient:  703707.8925562726\n",
      "Iteration: 271, Norm of Gradient:  691199.6367954994\n",
      "Iteration: 272, Norm of Gradient:  678913.7125788658\n",
      "Iteration: 273, Norm of Gradient:  666846.1680112672\n",
      "Iteration: 274, Norm of Gradient:  654993.1214413629\n",
      "Iteration: 275, Norm of Gradient:  643350.7602139674\n",
      "Iteration: 276, Norm of Gradient:  631915.3394418233\n",
      "Iteration: 277, Norm of Gradient:  620683.1808039875\n",
      "Iteration: 278, Norm of Gradient:  609650.6713591247\n",
      "Iteration: 279, Norm of Gradient:  598814.2623861739\n",
      "Iteration: 280, Norm of Gradient:  588170.4682417905\n",
      "Iteration: 281, Norm of Gradient:  577715.8652390144\n",
      "Iteration: 282, Norm of Gradient:  567447.0905457461\n",
      "Iteration: 283, Norm of Gradient:  557360.8411048867\n",
      "Iteration: 284, Norm of Gradient:  547453.8725683747\n",
      "Iteration: 285, Norm of Gradient:  537722.9982573969\n",
      "Iteration: 286, Norm of Gradient:  528165.0881348082\n",
      "Iteration: 287, Norm of Gradient:  518777.06779959414\n",
      "Iteration: 288, Norm of Gradient:  509555.9174980585\n",
      "Iteration: 289, Norm of Gradient:  500498.67115076486\n",
      "Iteration: 290, Norm of Gradient:  491602.4154010238\n",
      "Iteration: 291, Norm of Gradient:  482864.288674668\n",
      "Iteration: 292, Norm of Gradient:  474281.4802633613\n",
      "Iteration: 293, Norm of Gradient:  465851.229417399\n",
      "Iteration: 294, Norm of Gradient:  457570.8244586076\n",
      "Iteration: 295, Norm of Gradient:  449437.6019095801\n",
      "Iteration: 296, Norm of Gradient:  441448.9456339196\n",
      "Iteration: 297, Norm of Gradient:  433602.28599722404\n",
      "Iteration: 298, Norm of Gradient:  425895.099040858\n",
      "Iteration: 299, Norm of Gradient:  418324.90566768125\n",
      "Iteration: 300, Norm of Gradient:  410889.270845749\n",
      "Iteration: 301, Norm of Gradient:  403585.8028264409\n",
      "Iteration: 302, Norm of Gradient:  396412.15237369656\n",
      "Iteration: 303, Norm of Gradient:  389366.01200812927\n",
      "Iteration: 304, Norm of Gradient:  382445.115264609\n",
      "Iteration: 305, Norm of Gradient:  375647.2359658191\n",
      "Iteration: 306, Norm of Gradient:  368970.1875028103\n",
      "Iteration: 307, Norm of Gradient:  362411.82213385263\n",
      "Iteration: 308, Norm of Gradient:  355970.0302925682\n",
      "Iteration: 309, Norm of Gradient:  349642.73990938324\n",
      "Iteration: 310, Norm of Gradient:  343427.9157452946\n",
      "Iteration: 311, Norm of Gradient:  337323.5587384993\n",
      "Iteration: 312, Norm of Gradient:  331327.7053589292\n",
      "Iteration: 313, Norm of Gradient:  325438.426978044\n",
      "Iteration: 314, Norm of Gradient:  319653.8292470473\n",
      "Iteration: 315, Norm of Gradient:  313972.05149121134\n",
      "Iteration: 316, Norm of Gradient:  308391.26610770385\n",
      "Iteration: 317, Norm of Gradient:  302909.67797905864\n",
      "Iteration: 318, Norm of Gradient:  297525.5238947648\n",
      "Iteration: 319, Norm of Gradient:  292237.07198605913\n",
      "Iteration: 320, Norm of Gradient:  287042.62116782326\n",
      "Iteration: 321, Norm of Gradient:  281940.5005901512\n",
      "Iteration: 322, Norm of Gradient:  276929.06910394685\n",
      "Iteration: 323, Norm of Gradient:  272006.71472943283\n",
      "Iteration: 324, Norm of Gradient:  267171.85414046684\n",
      "Iteration: 325, Norm of Gradient:  262422.9321541359\n",
      "Iteration: 326, Norm of Gradient:  257758.42123016884\n",
      "Iteration: 327, Norm of Gradient:  253176.82097956343\n",
      "Iteration: 328, Norm of Gradient:  248676.65768428633\n",
      "Iteration: 329, Norm of Gradient:  244256.48381867644\n",
      "Iteration: 330, Norm of Gradient:  239914.8775893007\n",
      "Iteration: 331, Norm of Gradient:  235650.44247212817\n",
      "Iteration: 332, Norm of Gradient:  231461.80676870237\n",
      "Iteration: 333, Norm of Gradient:  227347.62315909742\n",
      "Iteration: 334, Norm of Gradient:  223306.56827442453\n",
      "Iteration: 335, Norm of Gradient:  219337.34226740655\n",
      "Iteration: 336, Norm of Gradient:  215438.66839491733\n",
      "Iteration: 337, Norm of Gradient:  211609.29260846841\n",
      "Iteration: 338, Norm of Gradient:  207847.98314937306\n",
      "Iteration: 339, Norm of Gradient:  204153.5301531244\n",
      "Iteration: 340, Norm of Gradient:  200524.74526095574\n",
      "Iteration: 341, Norm of Gradient:  196960.4612362481\n",
      "Iteration: 342, Norm of Gradient:  193459.53158989715\n",
      "Iteration: 343, Norm of Gradient:  190020.83021146726\n",
      "Iteration: 344, Norm of Gradient:  186643.2510074379\n",
      "Iteration: 345, Norm of Gradient:  183325.70754400056\n",
      "Iteration: 346, Norm of Gradient:  180067.13269892082\n",
      "Iteration: 347, Norm of Gradient:  176866.47831760743\n",
      "Iteration: 348, Norm of Gradient:  173722.7148766589\n",
      "Iteration: 349, Norm of Gradient:  170634.83115132473\n",
      "Iteration: 350, Norm of Gradient:  167601.83389198396\n",
      "Iteration: 351, Norm of Gradient:  164622.74750399814\n",
      "Iteration: 352, Norm of Gradient:  161696.61373287346\n",
      "Iteration: 353, Norm of Gradient:  158822.4913574272\n",
      "Iteration: 354, Norm of Gradient:  155999.4558861766\n",
      "Iteration: 355, Norm of Gradient:  153226.59926032135\n",
      "Iteration: 356, Norm of Gradient:  150503.02956296323\n",
      "Iteration: 357, Norm of Gradient:  147827.8707271868\n",
      "Iteration: 358, Norm of Gradient:  145200.2622625646\n",
      "Iteration: 359, Norm of Gradient:  142619.3589703546\n",
      "Iteration: 360, Norm of Gradient:  140084.3306768666\n",
      "Iteration: 361, Norm of Gradient:  137594.36196342527\n",
      "Iteration: 362, Norm of Gradient:  135148.65190608794\n",
      "Iteration: 363, Norm of Gradient:  132746.41381641466\n",
      "Iteration: 364, Norm of Gradient:  130386.87498986394\n",
      "Iteration: 365, Norm of Gradient:  128069.27645622434\n",
      "Iteration: 366, Norm of Gradient:  125792.87273594218\n",
      "Iteration: 367, Norm of Gradient:  123556.93159970369\n",
      "Iteration: 368, Norm of Gradient:  121360.7338345258\n",
      "Iteration: 369, Norm of Gradient:  119203.57300993601\n",
      "Iteration: 370, Norm of Gradient:  117084.75525295487\n",
      "Iteration: 371, Norm of Gradient:  115003.59902338784\n",
      "Iteration: 372, Norm of Gradient:  112959.43489654176\n",
      "Iteration: 373, Norm of Gradient:  110951.60534598927\n",
      "Iteration: 374, Norm of Gradient:  108979.46453116812\n",
      "Iteration: 375, Norm of Gradient:  107042.37809368318\n",
      "Iteration: 376, Norm of Gradient:  105139.72294852734\n",
      "Iteration: 377, Norm of Gradient:  103270.88708800351\n",
      "Iteration: 378, Norm of Gradient:  101435.26938127729\n",
      "Iteration: 379, Norm of Gradient:  99632.27938244169\n",
      "Iteration: 380, Norm of Gradient:  97861.33714126449\n",
      "Iteration: 381, Norm of Gradient:  96121.8730157537\n",
      "Iteration: 382, Norm of Gradient:  94413.32748965657\n",
      "Iteration: 383, Norm of Gradient:  92735.15099128452\n",
      "Iteration: 384, Norm of Gradient:  91086.80371719692\n",
      "Iteration: 385, Norm of Gradient:  89467.75545989575\n",
      "Iteration: 386, Norm of Gradient:  87877.4854355527\n",
      "Iteration: 387, Norm of Gradient:  86315.4821172156\n",
      "Iteration: 388, Norm of Gradient:  84781.24307063894\n",
      "Iteration: 389, Norm of Gradient:  83274.27479173156\n",
      "Iteration: 390, Norm of Gradient:  81794.09254803133\n",
      "Iteration: 391, Norm of Gradient:  80340.22022409076\n",
      "Iteration: 392, Norm of Gradient:  78912.19016638618\n",
      "Iteration: 393, Norm of Gradient:  77509.54303484362\n",
      "Iteration: 394, Norm of Gradient:  76131.82765305297\n",
      "Iteration: 395, Norm of Gradient:  74778.60086436903\n",
      "Iteration: 396, Norm of Gradient:  73449.42738918401\n",
      "Iteration: 397, Norm of Gradient:  72143.87968563734\n",
      "Iteration: 398, Norm of Gradient:  70861.5378109091\n",
      "Iteration: 399, Norm of Gradient:  69601.98928492876\n",
      "Iteration: 400, Norm of Gradient:  68364.82896270089\n",
      "Iteration: 401, Norm of Gradient:  67149.65889800117\n",
      "Iteration: 402, Norm of Gradient:  65956.0882181201\n",
      "Iteration: 403, Norm of Gradient:  64783.732999684726\n",
      "Iteration: 404, Norm of Gradient:  63632.21614146487\n",
      "Iteration: 405, Norm of Gradient:  62501.167246962024\n",
      "Iteration: 406, Norm of Gradient:  61390.222502087876\n",
      "Iteration: 407, Norm of Gradient:  60299.02456035344\n",
      "Iteration: 408, Norm of Gradient:  59227.2224260341\n",
      "Iteration: 409, Norm of Gradient:  58174.471342972385\n",
      "Iteration: 410, Norm of Gradient:  57140.43268308721\n",
      "Iteration: 411, Norm of Gradient:  56124.77383727833\n",
      "Iteration: 412, Norm of Gradient:  55127.16810750067\n",
      "Iteration: 413, Norm of Gradient:  54147.29460449694\n",
      "Iteration: 414, Norm of Gradient:  53184.83814108744\n",
      "Iteration: 415, Norm of Gradient:  52239.48913326017\n",
      "Iteration: 416, Norm of Gradient:  51310.94350003876\n",
      "Iteration: 417, Norm of Gradient:  50398.90256513693\n",
      "Iteration: 418, Norm of Gradient:  49503.072960249185\n",
      "Iteration: 419, Norm of Gradient:  48623.16653323681\n",
      "Iteration: 420, Norm of Gradient:  47758.90025271707\n",
      "Iteration: 421, Norm of Gradient:  46909.99611895332\n",
      "Iteration: 422, Norm of Gradient:  46076.18107227033\n",
      "Iteration: 423, Norm of Gradient:  45257.18690837534\n",
      "Iteration: 424, Norm of Gradient:  44452.750188171216\n",
      "Iteration: 425, Norm of Gradient:  43662.612157812466\n",
      "Iteration: 426, Norm of Gradient:  42886.518659624526\n",
      "Iteration: 427, Norm of Gradient:  42124.22005552737\n",
      "Iteration: 428, Norm of Gradient:  41375.47114483645\n",
      "Iteration: 429, Norm of Gradient:  40640.03108450765\n",
      "Iteration: 430, Norm of Gradient:  39917.663311894605\n",
      "Iteration: 431, Norm of Gradient:  39208.13547031132\n",
      "Iteration: 432, Norm of Gradient:  38511.21933304936\n",
      "Iteration: 433, Norm of Gradient:  37826.69072973949\n",
      "Iteration: 434, Norm of Gradient:  37154.32947424401\n",
      "Iteration: 435, Norm of Gradient:  36493.91929500269\n",
      "Iteration: 436, Norm of Gradient:  35845.24776341093\n",
      "Iteration: 437, Norm of Gradient:  35208.106228922996\n",
      "Iteration: 438, Norm of Gradient:  34582.28974698661\n",
      "Iteration: 439, Norm of Gradient:  33967.59701845059\n",
      "Iteration: 440, Norm of Gradient:  33363.830319578905\n",
      "Iteration: 441, Norm of Gradient:  32770.79544379637\n",
      "Iteration: 442, Norm of Gradient:  32188.30163423846\n",
      "Iteration: 443, Norm of Gradient:  31616.16152692347\n",
      "Iteration: 444, Norm of Gradient:  31054.1910865997\n",
      "Iteration: 445, Norm of Gradient:  30502.209548714265\n",
      "Iteration: 446, Norm of Gradient:  29960.039363852004\n",
      "Iteration: 447, Norm of Gradient:  29427.506137128108\n",
      "Iteration: 448, Norm of Gradient:  28904.438573080617\n",
      "Iteration: 449, Norm of Gradient:  28390.668422062274\n",
      "Iteration: 450, Norm of Gradient:  27886.03042463955\n",
      "Iteration: 451, Norm of Gradient:  27390.362258187917\n",
      "Iteration: 452, Norm of Gradient:  26903.504486977115\n",
      "Iteration: 453, Norm of Gradient:  26425.300506649477\n",
      "Iteration: 454, Norm of Gradient:  25955.59649939799\n",
      "Iteration: 455, Norm of Gradient:  25494.241379293548\n",
      "Iteration: 456, Norm of Gradient:  25041.086746520003\n",
      "Iteration: 457, Norm of Gradient:  24595.986839823658\n",
      "Iteration: 458, Norm of Gradient:  24158.798487374886\n",
      "Iteration: 459, Norm of Gradient:  23729.381063427863\n",
      "Iteration: 460, Norm of Gradient:  23307.596441722988\n",
      "Iteration: 461, Norm of Gradient:  22893.30895021591\n",
      "Iteration: 462, Norm of Gradient:  22486.38532993793\n",
      "Iteration: 463, Norm of Gradient:  22086.69468868738\n",
      "Iteration: 464, Norm of Gradient:  21694.108461879052\n",
      "Iteration: 465, Norm of Gradient:  21308.500371065293\n",
      "Iteration: 466, Norm of Gradient:  20929.746380688382\n",
      "Iteration: 467, Norm of Gradient:  20557.724660440097\n",
      "Iteration: 468, Norm of Gradient:  20192.315546049533\n",
      "Iteration: 469, Norm of Gradient:  19833.40149953931\n",
      "Iteration: 470, Norm of Gradient:  19480.86707300203\n",
      "Iteration: 471, Norm of Gradient:  19134.598869282592\n",
      "Iteration: 472, Norm of Gradient:  18794.4855083505\n",
      "Iteration: 473, Norm of Gradient:  18460.417588805325\n",
      "Iteration: 474, Norm of Gradient:  18132.287654471886\n",
      "Iteration: 475, Norm of Gradient:  17809.990158911965\n",
      "Iteration: 476, Norm of Gradient:  17493.421431225433\n",
      "Iteration: 477, Norm of Gradient:  17182.479644692346\n",
      "Iteration: 478, Norm of Gradient:  16877.064781585177\n",
      "Iteration: 479, Norm of Gradient:  16577.078601496105\n",
      "Iteration: 480, Norm of Gradient:  16282.424610499942\n",
      "Iteration: 481, Norm of Gradient:  15993.00803090277\n",
      "Iteration: 482, Norm of Gradient:  15708.735768228238\n",
      "Iteration: 483, Norm of Gradient:  15429.516383495622\n",
      "Iteration: 484, Norm of Gradient:  15155.260063323776\n",
      "Iteration: 485, Norm of Gradient:  14885.878589684013\n",
      "Iteration: 486, Norm of Gradient:  14621.285314057812\n",
      "Iteration: 487, Norm of Gradient:  14361.395126383231\n",
      "Iteration: 488, Norm of Gradient:  14106.124430653561\n",
      "Iteration: 489, Norm of Gradient:  13855.391116465831\n",
      "Iteration: 490, Norm of Gradient:  13609.114532758536\n",
      "Iteration: 491, Norm of Gradient:  13367.215462280637\n",
      "Iteration: 492, Norm of Gradient:  13129.616095430025\n",
      "Iteration: 493, Norm of Gradient:  12896.240005736312\n",
      "Iteration: 494, Norm of Gradient:  12667.01212555539\n",
      "Iteration: 495, Norm of Gradient:  12441.858721817212\n",
      "Iteration: 496, Norm of Gradient:  12220.707370749098\n",
      "Iteration: 497, Norm of Gradient:  12003.4869380011\n",
      "Iteration: 498, Norm of Gradient:  11790.127550326919\n",
      "Iteration: 499, Norm of Gradient:  11580.560580920308\n",
      "Iteration: 500, Norm of Gradient:  11374.718618325973\n",
      "Iteration: 501, Norm of Gradient:  11172.53545201364\n",
      "Iteration: 502, Norm of Gradient:  10973.946048555863\n",
      "Iteration: 503, Norm of Gradient:  10778.88652839898\n",
      "Iteration: 504, Norm of Gradient:  10587.29414883947\n",
      "Iteration: 505, Norm of Gradient:  10399.107282693582\n",
      "Iteration: 506, Norm of Gradient:  10214.26539762415\n",
      "Iteration: 507, Norm of Gradient:  10032.70903684683\n",
      "Iteration: 508, Norm of Gradient:  9854.379800934861\n",
      "Iteration: 509, Norm of Gradient:  9679.220329289132\n",
      "Iteration: 510, Norm of Gradient:  9507.174278784025\n",
      "Iteration: 511, Norm of Gradient:  9338.18631007376\n",
      "Iteration: 512, Norm of Gradient:  9172.202066004287\n",
      "Iteration: 513, Norm of Gradient:  9009.168155713838\n",
      "Iteration: 514, Norm of Gradient:  8849.032137817056\n",
      "Iteration: 515, Norm of Gradient:  8691.742503356552\n",
      "Iteration: 516, Norm of Gradient:  8537.24865859635\n",
      "Iteration: 517, Norm of Gradient:  8385.500907900987\n",
      "Iteration: 518, Norm of Gradient:  8236.450440889881\n",
      "Iteration: 519, Norm of Gradient:  8090.049313778283\n",
      "Iteration: 520, Norm of Gradient:  7946.250435161078\n",
      "Iteration: 521, Norm of Gradient:  7805.007550748746\n",
      "Iteration: 522, Norm of Gradient:  7666.275227436023\n",
      "Iteration: 523, Norm of Gradient:  7530.008841327993\n",
      "Iteration: 524, Norm of Gradient:  7396.164561123996\n",
      "Iteration: 525, Norm of Gradient:  7264.699333918682\n",
      "Iteration: 526, Norm of Gradient:  7135.570872868826\n",
      "Iteration: 527, Norm of Gradient:  7008.737642028109\n",
      "Iteration: 528, Norm of Gradient:  6884.158844430763\n",
      "Iteration: 529, Norm of Gradient:  6761.794408217876\n",
      "Iteration: 530, Norm of Gradient:  6641.604973565286\n",
      "Iteration: 531, Norm of Gradient:  6523.551880376855\n",
      "Iteration: 532, Norm of Gradient:  6407.597154610288\n",
      "Iteration: 533, Norm of Gradient:  6293.703500045299\n",
      "Iteration: 534, Norm of Gradient:  6181.834279849692\n",
      "Iteration: 535, Norm of Gradient:  6071.953511055528\n",
      "Iteration: 536, Norm of Gradient:  5964.025849365464\n",
      "Iteration: 537, Norm of Gradient:  5858.01657833408\n",
      "Iteration: 538, Norm of Gradient:  5753.891599564314\n",
      "Iteration: 539, Norm of Gradient:  5651.617419108141\n",
      "Iteration: 540, Norm of Gradient:  5551.161140281004\n",
      "Iteration: 541, Norm of Gradient:  5452.490450031568\n",
      "Iteration: 542, Norm of Gradient:  5355.57361089362\n",
      "Iteration: 543, Norm of Gradient:  5260.379447257156\n",
      "Iteration: 544, Norm of Gradient:  5166.877338896201\n",
      "Iteration: 545, Norm of Gradient:  5075.037210158632\n",
      "Iteration: 546, Norm of Gradient:  4984.829520080615\n",
      "Iteration: 547, Norm of Gradient:  4896.225252156774\n",
      "Iteration: 548, Norm of Gradient:  4809.1959065948295\n",
      "Iteration: 549, Norm of Gradient:  4723.713488044064\n",
      "Iteration: 550, Norm of Gradient:  4639.750501246903\n",
      "Iteration: 551, Norm of Gradient:  4557.279938488811\n",
      "Iteration: 552, Norm of Gradient:  4476.275272192189\n",
      "Iteration: 553, Norm of Gradient:  4396.710445946293\n",
      "Iteration: 554, Norm of Gradient:  4318.559868463477\n",
      "Iteration: 555, Norm of Gradient:  4241.798399492206\n",
      "Iteration: 556, Norm of Gradient:  4166.4013492024615\n",
      "Iteration: 557, Norm of Gradient:  4092.3444653266033\n",
      "Iteration: 558, Norm of Gradient:  4019.6039266885405\n",
      "Iteration: 559, Norm of Gradient:  3948.1563354796085\n",
      "Iteration: 560, Norm of Gradient:  3877.9787094340336\n",
      "Iteration: 561, Norm of Gradient:  3809.0484755508605\n",
      "Iteration: 562, Norm of Gradient:  3741.3434617895273\n",
      "Iteration: 563, Norm of Gradient:  3674.841890082245\n",
      "Iteration: 564, Norm of Gradient:  3609.522370011763\n",
      "Iteration: 565, Norm of Gradient:  3545.3638890427605\n",
      "Iteration: 566, Norm of Gradient:  3482.3458120565147\n",
      "Iteration: 567, Norm of Gradient:  3420.447867635794\n",
      "Iteration: 568, Norm of Gradient:  3359.6501456307487\n",
      "Iteration: 569, Norm of Gradient:  3299.933090329637\n",
      "Iteration: 570, Norm of Gradient:  3241.277492329611\n",
      "Iteration: 571, Norm of Gradient:  3183.664485556112\n",
      "Iteration: 572, Norm of Gradient:  3127.0755366740823\n",
      "Iteration: 573, Norm of Gradient:  3071.492444955076\n",
      "Iteration: 574, Norm of Gradient:  3016.8973300491357\n",
      "Iteration: 575, Norm of Gradient:  2963.2726317950824\n",
      "Iteration: 576, Norm of Gradient:  2910.601101305103\n",
      "Iteration: 577, Norm of Gradient:  2858.8657954076193\n",
      "Iteration: 578, Norm of Gradient:  2808.050073528137\n",
      "Iteration: 579, Norm of Gradient:  2758.1375903587923\n",
      "Iteration: 580, Norm of Gradient:  2709.1122905751777\n",
      "Iteration: 581, Norm of Gradient:  2660.958404538112\n",
      "Iteration: 582, Norm of Gradient:  2613.6604437787446\n",
      "Iteration: 583, Norm of Gradient:  2567.2031940932225\n",
      "Iteration: 584, Norm of Gradient:  2521.5717121152843\n",
      "Iteration: 585, Norm of Gradient:  2476.7513197867934\n",
      "Iteration: 586, Norm of Gradient:  2432.7276000017628\n",
      "Iteration: 587, Norm of Gradient:  2389.486392662681\n",
      "Iteration: 588, Norm of Gradient:  2347.0137879213\n",
      "Iteration: 589, Norm of Gradient:  2305.2961251334327\n",
      "Iteration: 590, Norm of Gradient:  2264.3199845980394\n",
      "Iteration: 591, Norm of Gradient:  2224.072185912908\n",
      "Iteration: 592, Norm of Gradient:  2184.5397832770404\n",
      "Iteration: 593, Norm of Gradient:  2145.7100606854415\n",
      "Iteration: 594, Norm of Gradient:  2107.5705277513853\n",
      "Iteration: 595, Norm of Gradient:  2070.1089169494544\n",
      "Iteration: 596, Norm of Gradient:  2033.3131783687377\n",
      "Iteration: 597, Norm of Gradient:  1997.171475831777\n",
      "Iteration: 598, Norm of Gradient:  1961.672184326244\n",
      "Iteration: 599, Norm of Gradient:  1926.8038848384215\n",
      "Iteration: 600, Norm of Gradient:  1892.555362081605\n",
      "Iteration: 601, Norm of Gradient:  1858.9155994565292\n",
      "Iteration: 602, Norm of Gradient:  1825.873776117573\n",
      "Iteration: 603, Norm of Gradient:  1793.4192644108523\n",
      "Iteration: 604, Norm of Gradient:  1761.5416245370002\n",
      "Iteration: 605, Norm of Gradient:  1730.2306026308042\n",
      "Iteration: 606, Norm of Gradient:  1699.476127342369\n",
      "Iteration: 607, Norm of Gradient:  1669.2683065865222\n",
      "Iteration: 608, Norm of Gradient:  1639.5974230615648\n",
      "Iteration: 609, Norm of Gradient:  1610.4539331851197\n",
      "Iteration: 610, Norm of Gradient:  1581.8284621479477\n",
      "Iteration: 611, Norm of Gradient:  1553.711803299395\n",
      "Iteration: 612, Norm of Gradient:  1526.0949119535342\n",
      "Iteration: 613, Norm of Gradient:  1498.9689042508314\n",
      "Iteration: 614, Norm of Gradient:  1472.325055462062\n",
      "Iteration: 615, Norm of Gradient:  1446.1547961972994\n",
      "Iteration: 616, Norm of Gradient:  1420.4497067785328\n",
      "Iteration: 617, Norm of Gradient:  1395.2015198285021\n",
      "Iteration: 618, Norm of Gradient:  1370.4021147512926\n",
      "Iteration: 619, Norm of Gradient:  1346.0435132601876\n",
      "Iteration: 620, Norm of Gradient:  1322.1178804408069\n",
      "Iteration: 621, Norm of Gradient:  1298.6175206421908\n",
      "Iteration: 622, Norm of Gradient:  1275.534875083262\n",
      "Iteration: 623, Norm of Gradient:  1252.8625180889128\n",
      "Iteration: 624, Norm of Gradient:  1230.5931577010142\n",
      "Iteration: 625, Norm of Gradient:  1208.7196298599586\n",
      "Iteration: 626, Norm of Gradient:  1187.2348997691806\n",
      "Iteration: 627, Norm of Gradient:  1166.1320556779249\n",
      "Iteration: 628, Norm of Gradient:  1145.4043104950742\n",
      "Iteration: 629, Norm of Gradient:  1125.044996028708\n",
      "Iteration: 630, Norm of Gradient:  1105.0475644849014\n",
      "Iteration: 631, Norm of Gradient:  1085.4055829867043\n",
      "Iteration: 632, Norm of Gradient:  1066.1127332194783\n",
      "Iteration: 633, Norm of Gradient:  1047.16280911498\n",
      "Iteration: 634, Norm of Gradient:  1028.5497164761316\n",
      "Iteration: 635, Norm of Gradient:  1010.2674678488341\n",
      "Iteration: 636, Norm of Gradient:  992.3101821280729\n",
      "Iteration: 637, Norm of Gradient:  974.6720828408095\n",
      "Iteration: 638, Norm of Gradient:  957.3474968411113\n",
      "Iteration: 639, Norm of Gradient:  940.3308518979068\n",
      "Iteration: 640, Norm of Gradient:  923.616674421716\n",
      "Iteration: 641, Norm of Gradient:  907.1995874430946\n",
      "Iteration: 642, Norm of Gradient:  891.074310797023\n",
      "Iteration: 643, Norm of Gradient:  875.2356575695684\n",
      "Iteration: 644, Norm of Gradient:  859.6785325994133\n",
      "Iteration: 645, Norm of Gradient:  844.3979325568996\n",
      "Iteration: 646, Norm of Gradient:  829.3889414797572\n",
      "Iteration: 647, Norm of Gradient:  814.6467324230246\n",
      "Iteration: 648, Norm of Gradient:  800.1665630736923\n",
      "Iteration: 649, Norm of Gradient:  785.9437747390241\n",
      "Iteration: 650, Norm of Gradient:  771.9737941570334\n",
      "Iteration: 651, Norm of Gradient:  758.2521273260996\n",
      "Iteration: 652, Norm of Gradient:  744.774359514355\n",
      "Iteration: 653, Norm of Gradient:  731.5361562032895\n",
      "Iteration: 654, Norm of Gradient:  718.5332583052718\n",
      "Iteration: 655, Norm of Gradient:  705.7614854195298\n",
      "Iteration: 656, Norm of Gradient:  693.2167273031353\n",
      "Iteration: 657, Norm of Gradient:  680.894949834601\n",
      "Iteration: 658, Norm of Gradient:  668.7921892505058\n",
      "Iteration: 659, Norm of Gradient:  656.9045526026966\n",
      "Iteration: 660, Norm of Gradient:  645.2282157746715\n",
      "Iteration: 661, Norm of Gradient:  633.7594239850488\n",
      "Iteration: 662, Norm of Gradient:  622.4944870112454\n",
      "Iteration: 663, Norm of Gradient:  611.429781988616\n",
      "Iteration: 664, Norm of Gradient:  600.5617495776489\n",
      "Iteration: 665, Norm of Gradient:  589.886894432498\n",
      "Iteration: 666, Norm of Gradient:  579.4017823937644\n",
      "Iteration: 667, Norm of Gradient:  569.103040990121\n",
      "Iteration: 668, Norm of Gradient:  558.9873569457728\n",
      "Iteration: 669, Norm of Gradient:  549.0514774316357\n",
      "Iteration: 670, Norm of Gradient:  539.2922059329736\n",
      "Iteration: 671, Norm of Gradient:  529.7064038884014\n",
      "Iteration: 672, Norm of Gradient:  520.2909867659955\n",
      "Iteration: 673, Norm of Gradient:  511.0429270055085\n",
      "Iteration: 674, Norm of Gradient:  501.9592494061309\n",
      "Iteration: 675, Norm of Gradient:  493.0370322755406\n",
      "Iteration: 676, Norm of Gradient:  484.2734054698809\n",
      "Iteration: 677, Norm of Gradient:  475.6655506392645\n",
      "Iteration: 678, Norm of Gradient:  467.2106977336482\n",
      "Iteration: 679, Norm of Gradient:  458.9061287663473\n",
      "Iteration: 680, Norm of Gradient:  450.7491709187964\n",
      "Iteration: 681, Norm of Gradient:  442.73720258414085\n",
      "Iteration: 682, Norm of Gradient:  434.8676439406139\n",
      "Iteration: 683, Norm of Gradient:  427.137965697731\n",
      "Iteration: 684, Norm of Gradient:  419.545680454724\n",
      "Iteration: 685, Norm of Gradient:  412.088346650036\n",
      "Iteration: 686, Norm of Gradient:  404.7635656349286\n",
      "Iteration: 687, Norm of Gradient:  397.56898084791175\n",
      "Iteration: 688, Norm of Gradient:  390.5022780777755\n",
      "Iteration: 689, Norm of Gradient:  383.5611841989491\n",
      "Iteration: 690, Norm of Gradient:  376.7434673391968\n",
      "Iteration: 691, Norm of Gradient:  370.0469335032162\n",
      "Iteration: 692, Norm of Gradient:  363.46942897169805\n",
      "Iteration: 693, Norm of Gradient:  357.00883858932804\n",
      "Iteration: 694, Norm of Gradient:  350.66308363043703\n",
      "Iteration: 695, Norm of Gradient:  344.43012295837946\n",
      "Iteration: 696, Norm of Gradient:  338.30795163880094\n",
      "Iteration: 697, Norm of Gradient:  332.2946013826928\n",
      "Iteration: 698, Norm of Gradient:  326.38813658442945\n",
      "Iteration: 699, Norm of Gradient:  320.5866576995207\n",
      "Iteration: 700, Norm of Gradient:  314.88829907590457\n",
      "Iteration: 701, Norm of Gradient:  309.2912277029523\n",
      "Iteration: 702, Norm of Gradient:  303.7936427213946\n",
      "Iteration: 703, Norm of Gradient:  298.39377658163573\n",
      "Iteration: 704, Norm of Gradient:  293.08989156804574\n",
      "Iteration: 705, Norm of Gradient:  287.8802818719588\n",
      "Iteration: 706, Norm of Gradient:  282.7632723092724\n",
      "Iteration: 707, Norm of Gradient:  277.7372162517298\n",
      "Iteration: 708, Norm of Gradient:  272.8004973509213\n",
      "Iteration: 709, Norm of Gradient:  267.9515273442362\n",
      "Iteration: 710, Norm of Gradient:  263.1887469804769\n",
      "Iteration: 711, Norm of Gradient:  258.51062341323467\n",
      "Iteration: 712, Norm of Gradient:  253.9156531981476\n",
      "Iteration: 713, Norm of Gradient:  249.4023572556693\n",
      "Iteration: 714, Norm of Gradient:  244.96928422208595\n",
      "Iteration: 715, Norm of Gradient:  240.61500792290687\n",
      "Iteration: 716, Norm of Gradient:  236.3381285172878\n",
      "Iteration: 717, Norm of Gradient:  232.1372690361047\n",
      "Iteration: 718, Norm of Gradient:  228.01107930379632\n",
      "Iteration: 719, Norm of Gradient:  223.95823183721998\n",
      "Iteration: 720, Norm of Gradient:  219.97742278326712\n",
      "Iteration: 721, Norm of Gradient:  216.0673714773914\n",
      "Iteration: 722, Norm of Gradient:  212.22682077051795\n",
      "Iteration: 723, Norm of Gradient:  208.4545345518993\n",
      "Iteration: 724, Norm of Gradient:  204.74930025466975\n",
      "Iteration: 725, Norm of Gradient:  201.10992564289958\n",
      "Iteration: 726, Norm of Gradient:  197.5352402378993\n",
      "Iteration: 727, Norm of Gradient:  194.02409415077477\n",
      "Iteration: 728, Norm of Gradient:  190.57535787105732\n",
      "Iteration: 729, Norm of Gradient:  187.1879219973016\n",
      "Iteration: 730, Norm of Gradient:  183.86069739597346\n",
      "Iteration: 731, Norm of Gradient:  180.59261289002632\n",
      "Iteration: 732, Norm of Gradient:  177.38261838284834\n",
      "Iteration: 733, Norm of Gradient:  174.2296805077559\n",
      "Iteration: 734, Norm of Gradient:  171.1327853155724\n",
      "Iteration: 735, Norm of Gradient:  168.090937235347\n",
      "Iteration: 736, Norm of Gradient:  165.10315645491016\n",
      "Iteration: 737, Norm of Gradient:  162.1684841752014\n",
      "Iteration: 738, Norm of Gradient:  159.2859741806972\n",
      "Iteration: 739, Norm of Gradient:  156.45470082371844\n",
      "Iteration: 740, Norm of Gradient:  153.67375201802096\n",
      "Iteration: 741, Norm of Gradient:  150.942234170816\n",
      "Iteration: 742, Norm of Gradient:  148.25926893934997\n",
      "Iteration: 743, Norm of Gradient:  145.62399246753023\n",
      "Iteration: 744, Norm of Gradient:  143.0355573534565\n",
      "Iteration: 745, Norm of Gradient:  140.49313111336104\n",
      "Iteration: 746, Norm of Gradient:  137.99589623614642\n",
      "Iteration: 747, Norm of Gradient:  135.5430487258247\n",
      "Iteration: 748, Norm of Gradient:  133.13380022008917\n",
      "Iteration: 749, Norm of Gradient:  130.7673759265694\n",
      "Iteration: 750, Norm of Gradient:  128.44301430562118\n",
      "Iteration: 751, Norm of Gradient:  126.15996718874062\n",
      "Iteration: 752, Norm of Gradient:  123.91750120471305\n",
      "Iteration: 753, Norm of Gradient:  121.71489435215126\n",
      "Iteration: 754, Norm of Gradient:  119.55143825444932\n",
      "Iteration: 755, Norm of Gradient:  117.42643715148671\n",
      "Iteration: 756, Norm of Gradient:  115.33920793056875\n",
      "Iteration: 757, Norm of Gradient:  113.28907825563199\n",
      "Iteration: 758, Norm of Gradient:  111.27538937184556\n",
      "Iteration: 759, Norm of Gradient:  109.29749318020303\n",
      "Iteration: 760, Norm of Gradient:  107.35475422017491\n",
      "Iteration: 761, Norm of Gradient:  105.44654642727714\n",
      "Iteration: 762, Norm of Gradient:  103.57225690795487\n",
      "Iteration: 763, Norm of Gradient:  101.73128200728463\n",
      "Iteration: 764, Norm of Gradient:  99.92303105791221\n",
      "Iteration: 765, Norm of Gradient:  98.14692084968567\n",
      "Iteration: 766, Norm of Gradient:  96.40238051577272\n",
      "Iteration: 767, Norm of Gradient:  94.68884866367797\n",
      "Iteration: 768, Norm of Gradient:  93.00577488234346\n",
      "Iteration: 769, Norm of Gradient:  91.35261732375955\n",
      "Iteration: 770, Norm of Gradient:  89.72884459996524\n",
      "Iteration: 771, Norm of Gradient:  88.13393324205163\n",
      "Iteration: 772, Norm of Gradient:  86.5673721080408\n",
      "Iteration: 773, Norm of Gradient:  85.02865550650385\n",
      "Iteration: 774, Norm of Gradient:  83.5172896864469\n",
      "Iteration: 775, Norm of Gradient:  82.03278780921163\n",
      "Iteration: 776, Norm of Gradient:  80.57467274759382\n",
      "Iteration: 777, Norm of Gradient:  79.14247538659535\n",
      "Iteration: 778, Norm of Gradient:  77.73573484564531\n",
      "Iteration: 779, Norm of Gradient:  76.35399895094345\n",
      "Iteration: 780, Norm of Gradient:  74.99682346843875\n",
      "Iteration: 781, Norm of Gradient:  73.66377076105992\n",
      "Iteration: 782, Norm of Gradient:  72.35441346591298\n",
      "Iteration: 783, Norm of Gradient:  71.06832941059545\n",
      "Iteration: 784, Norm of Gradient:  69.8051049492449\n",
      "Iteration: 785, Norm of Gradient:  68.5643344849809\n",
      "Iteration: 786, Norm of Gradient:  67.34561810759584\n",
      "Iteration: 787, Norm of Gradient:  66.14856413694683\n",
      "Iteration: 788, Norm of Gradient:  64.97278806912841\n",
      "Iteration: 789, Norm of Gradient:  63.81791052974688\n",
      "Iteration: 790, Norm of Gradient:  62.68356129172959\n",
      "Iteration: 791, Norm of Gradient:  61.56937417835097\n",
      "Iteration: 792, Norm of Gradient:  60.47499178953948\n",
      "Iteration: 793, Norm of Gradient:  59.400061975874685\n",
      "Iteration: 794, Norm of Gradient:  58.34423895487526\n",
      "Iteration: 795, Norm of Gradient:  57.30718252725688\n",
      "Iteration: 796, Norm of Gradient:  56.288559749260365\n",
      "Iteration: 797, Norm of Gradient:  55.28804296973042\n",
      "Iteration: 798, Norm of Gradient:  54.30530972182179\n",
      "Iteration: 799, Norm of Gradient:  53.34004429214675\n",
      "Iteration: 800, Norm of Gradient:  52.39193684546833\n",
      "Iteration: 801, Norm of Gradient:  51.46068137140654\n",
      "Iteration: 802, Norm of Gradient:  50.545979050535124\n",
      "Iteration: 803, Norm of Gradient:  49.647535197613436\n",
      "Iteration: 804, Norm of Gradient:  48.76506104664819\n",
      "Iteration: 805, Norm of Gradient:  47.89827259351849\n",
      "Iteration: 806, Norm of Gradient:  47.04689108721137\n",
      "Iteration: 807, Norm of Gradient:  46.210642838129154\n",
      "Iteration: 808, Norm of Gradient:  45.38925856479892\n",
      "Iteration: 809, Norm of Gradient:  44.582473917914484\n",
      "Iteration: 810, Norm of Gradient:  43.79003013642737\n",
      "Iteration: 811, Norm of Gradient:  43.011672382644704\n",
      "Iteration: 812, Norm of Gradient:  42.24714873679147\n",
      "Iteration: 813, Norm of Gradient:  41.49621514788981\n",
      "Iteration: 814, Norm of Gradient:  40.75862851228984\n",
      "Iteration: 815, Norm of Gradient:  40.03415291894884\n",
      "Iteration: 816, Norm of Gradient:  39.322554629008124\n",
      "Iteration: 817, Norm of Gradient:  38.62360448223353\n",
      "Iteration: 818, Norm of Gradient:  37.937078299864844\n",
      "Iteration: 819, Norm of Gradient:  37.26275472680169\n",
      "Iteration: 820, Norm of Gradient:  36.60041747541926\n",
      "Iteration: 821, Norm of Gradient:  35.94985281373703\n",
      "Iteration: 822, Norm of Gradient:  35.310852171119194\n",
      "Iteration: 823, Norm of Gradient:  34.683209542917844\n",
      "Iteration: 824, Norm of Gradient:  34.06672281939301\n",
      "Iteration: 825, Norm of Gradient:  33.46119424220579\n",
      "Iteration: 826, Norm of Gradient:  32.86642857076674\n",
      "Iteration: 827, Norm of Gradient:  32.28223527897675\n",
      "Iteration: 828, Norm of Gradient:  31.708425450871886\n",
      "Iteration: 829, Norm of Gradient:  31.144814899816172\n",
      "Iteration: 830, Norm of Gradient:  30.59122252519526\n",
      "Iteration: 831, Norm of Gradient:  30.047470071421742\n",
      "Iteration: 832, Norm of Gradient:  29.513382635680568\n",
      "Iteration: 833, Norm of Gradient:  28.98878885458712\n",
      "Iteration: 834, Norm of Gradient:  28.473519277564588\n",
      "Iteration: 835, Norm of Gradient:  27.967408612144304\n",
      "Iteration: 836, Norm of Gradient:  27.470293660727823\n",
      "Iteration: 837, Norm of Gradient:  26.982015444931097\n",
      "Iteration: 838, Norm of Gradient:  26.50241558518899\n",
      "Iteration: 839, Norm of Gradient:  26.03134130464792\n",
      "Iteration: 840, Norm of Gradient:  25.568639765747857\n",
      "Iteration: 841, Norm of Gradient:  25.1141626720875\n",
      "Iteration: 842, Norm of Gradient:  24.66776400624709\n",
      "Iteration: 843, Norm of Gradient:  24.22929998592454\n",
      "Iteration: 844, Norm of Gradient:  23.798629431543635\n",
      "Iteration: 845, Norm of Gradient:  23.375613592694823\n",
      "Iteration: 846, Norm of Gradient:  22.960117378273377\n",
      "Iteration: 847, Norm of Gradient:  22.55200640613363\n",
      "Iteration: 848, Norm of Gradient:  22.15114910401811\n",
      "Iteration: 849, Norm of Gradient:  21.757417238180285\n",
      "Iteration: 850, Norm of Gradient:  21.37068392408521\n",
      "Iteration: 851, Norm of Gradient:  20.990824983691635\n",
      "Iteration: 852, Norm of Gradient:  20.617717263051997\n",
      "Iteration: 853, Norm of Gradient:  20.25124165642382\n",
      "Iteration: 854, Norm of Gradient:  19.89128028208826\n",
      "Iteration: 855, Norm of Gradient:  19.537716755090106\n",
      "Iteration: 856, Norm of Gradient:  19.190438477004374\n",
      "Iteration: 857, Norm of Gradient:  18.849332220620116\n",
      "Iteration: 858, Norm of Gradient:  18.514289695645015\n",
      "Iteration: 859, Norm of Gradient:  18.185202153964163\n",
      "Iteration: 860, Norm of Gradient:  17.86196416025914\n",
      "Iteration: 861, Norm of Gradient:  17.544471442505976\n",
      "Iteration: 862, Norm of Gradient:  17.232622354392703\n",
      "Iteration: 863, Norm of Gradient:  16.926316138038842\n",
      "Iteration: 864, Norm of Gradient:  16.625454939970737\n",
      "Iteration: 865, Norm of Gradient:  16.329940677990646\n",
      "Iteration: 866, Norm of Gradient:  16.03967954380781\n",
      "Iteration: 867, Norm of Gradient:  15.75457784527837\n",
      "Iteration: 868, Norm of Gradient:  15.474543648338313\n",
      "Iteration: 869, Norm of Gradient:  15.199486821973263\n",
      "Iteration: 870, Norm of Gradient:  14.929319206504694\n",
      "Iteration: 871, Norm of Gradient:  14.66395403874207\n",
      "Iteration: 872, Norm of Gradient:  14.403305024781242\n",
      "Iteration: 873, Norm of Gradient:  14.147289306604293\n",
      "Iteration: 874, Norm of Gradient:  13.895824920581264\n",
      "Iteration: 875, Norm of Gradient:  13.648829207695137\n",
      "Iteration: 876, Norm of Gradient:  13.406224298734811\n",
      "Iteration: 877, Norm of Gradient:  13.167931714527949\n",
      "Iteration: 878, Norm of Gradient:  12.933874276424575\n",
      "Iteration: 879, Norm of Gradient:  12.70397734540991\n",
      "Iteration: 880, Norm of Gradient:  12.47816654900266\n",
      "Iteration: 881, Norm of Gradient:  12.256370381971973\n",
      "Iteration: 882, Norm of Gradient:  12.038515976028853\n",
      "Iteration: 883, Norm of Gradient:  11.824534016488345\n",
      "Iteration: 884, Norm of Gradient:  11.614355375507518\n",
      "Iteration: 885, Norm of Gradient:  11.407912812058264\n",
      "Iteration: 886, Norm of Gradient:  11.205139451334691\n",
      "Iteration: 887, Norm of Gradient:  11.005970523643787\n",
      "Iteration: 888, Norm of Gradient:  10.810341938440866\n",
      "Iteration: 889, Norm of Gradient:  10.618190536516064\n",
      "Iteration: 890, Norm of Gradient:  10.429454389510477\n",
      "Iteration: 891, Norm of Gradient:  10.244072961466875\n",
      "Iteration: 892, Norm of Gradient:  10.061986835395796\n",
      "Iteration: 893, Norm of Gradient:  9.883137282905839\n",
      "Iteration: 894, Norm of Gradient:  9.707466539884177\n",
      "Iteration: 895, Norm of Gradient:  9.534918476394173\n",
      "Iteration: 896, Norm of Gradient:  9.365437664596545\n",
      "Iteration: 897, Norm of Gradient:  9.198968873543114\n",
      "Iteration: 898, Norm of Gradient:  9.035459518863323\n",
      "Iteration: 899, Norm of Gradient:  8.874856083742225\n",
      "Iteration: 900, Norm of Gradient:  8.717107142741831\n",
      "Iteration: 901, Norm of Gradient:  8.562162476448039\n",
      "Iteration: 902, Norm of Gradient:  8.409972098095201\n",
      "Iteration: 903, Norm of Gradient:  8.260486506093727\n",
      "Iteration: 904, Norm of Gradient:  8.113657778260137\n",
      "Iteration: 905, Norm of Gradient:  7.969439606464092\n",
      "Iteration: 906, Norm of Gradient:  7.827784556159464\n",
      "Iteration: 907, Norm of Gradient:  7.688647379599994\n",
      "Iteration: 908, Norm of Gradient:  7.551983281267524\n",
      "Iteration: 909, Norm of Gradient:  7.417748057570017\n",
      "Iteration: 910, Norm of Gradient:  7.285899434020269\n",
      "Iteration: 911, Norm of Gradient:  7.15639402097489\n",
      "Iteration: 912, Norm of Gradient:  7.029191043952406\n",
      "Iteration: 913, Norm of Gradient:  6.9042487048262355\n",
      "Iteration: 914, Norm of Gradient:  6.781527284492861\n",
      "Iteration: 915, Norm of Gradient:  6.66098662998352\n",
      "Iteration: 916, Norm of Gradient:  6.542589275969755\n",
      "Iteration: 917, Norm of Gradient:  6.426296084538746\n",
      "Iteration: 918, Norm of Gradient:  6.3120702869277165\n",
      "Iteration: 919, Norm of Gradient:  6.199874544521491\n",
      "Iteration: 920, Norm of Gradient:  6.0896732057364105\n",
      "Iteration: 921, Norm of Gradient:  5.981430069112403\n",
      "Iteration: 922, Norm of Gradient:  5.875111913888197\n",
      "Iteration: 923, Norm of Gradient:  5.770682859421925\n",
      "Iteration: 924, Norm of Gradient:  5.668110391808049\n",
      "Iteration: 925, Norm of Gradient:  5.567361237299905\n",
      "Iteration: 926, Norm of Gradient:  5.46840228402078\n",
      "Iteration: 927, Norm of Gradient:  5.371202385967187\n",
      "Iteration: 928, Norm of Gradient:  5.2757304510485845\n",
      "Iteration: 929, Norm of Gradient:  5.181955538954855\n",
      "Iteration: 930, Norm of Gradient:  5.089847621673682\n",
      "Iteration: 931, Norm of Gradient:  4.999376807447772\n",
      "Iteration: 932, Norm of Gradient:  4.910513709475241\n",
      "Iteration: 933, Norm of Gradient:  4.823230032219731\n",
      "Iteration: 934, Norm of Gradient:  4.737498587229651\n",
      "Iteration: 935, Norm of Gradient:  4.653290646845863\n",
      "Iteration: 936, Norm of Gradient:  4.570579348103284\n",
      "Iteration: 937, Norm of Gradient:  4.48933833473033\n",
      "Iteration: 938, Norm of Gradient:  4.40954073796014\n",
      "Iteration: 939, Norm of Gradient:  4.3311624949557865\n",
      "Iteration: 940, Norm of Gradient:  4.254176816091308\n",
      "Iteration: 941, Norm of Gradient:  4.178559793943642\n",
      "Iteration: 942, Norm of Gradient:  4.104286652437703\n",
      "Iteration: 943, Norm of Gradient:  4.031334352121834\n",
      "Iteration: 944, Norm of Gradient:  3.9596780358214625\n",
      "Iteration: 945, Norm of Gradient:  3.8892957885133357\n",
      "Iteration: 946, Norm of Gradient:  3.8201642755667216\n",
      "Iteration: 947, Norm of Gradient:  3.7522618105876746\n",
      "Iteration: 948, Norm of Gradient:  3.6855655317685194\n",
      "Iteration: 949, Norm of Gradient:  3.6200559128234535\n",
      "Iteration: 950, Norm of Gradient:  3.555710399500179\n",
      "Iteration: 951, Norm of Gradient:  3.4925080697862416\n",
      "Iteration: 952, Norm of Gradient:  3.4304291532734186\n",
      "Iteration: 953, Norm of Gradient:  3.3694541642180655\n",
      "Iteration: 954, Norm of Gradient:  3.3095631528228293\n",
      "Iteration: 955, Norm of Gradient:  3.250736041530482\n",
      "Iteration: 956, Norm of Gradient:  3.1929555252329536\n",
      "Iteration: 957, Norm of Gradient:  3.1362009162703846\n",
      "Iteration: 958, Norm of Gradient:  3.0804557631764538\n",
      "Iteration: 959, Norm of Gradient:  3.0257014898801575\n",
      "Iteration: 960, Norm of Gradient:  2.9719199964475456\n",
      "Iteration: 961, Norm of Gradient:  2.9190950960198014\n",
      "Iteration: 962, Norm of Gradient:  2.8672088254583583\n",
      "Iteration: 963, Norm of Gradient:  2.8162444311117967\n",
      "Iteration: 964, Norm of Gradient:  2.7661862331170486\n",
      "Iteration: 965, Norm of Gradient:  2.717018449124724\n",
      "Iteration: 966, Norm of Gradient:  2.6687236782030226\n",
      "Iteration: 967, Norm of Gradient:  2.6212877200200455\n",
      "Iteration: 968, Norm of Gradient:  2.574694978739468\n",
      "Iteration: 969, Norm of Gradient:  2.528930209836189\n",
      "Iteration: 970, Norm of Gradient:  2.4839791600131513\n",
      "Iteration: 971, Norm of Gradient:  2.4398268470237086\n",
      "Iteration: 972, Norm of Gradient:  2.3964595507114477\n",
      "Iteration: 973, Norm of Gradient:  2.3538629449480917\n",
      "Iteration: 974, Norm of Gradient:  2.3120230948007583\n",
      "Iteration: 975, Norm of Gradient:  2.270927624485044\n",
      "Iteration: 976, Norm of Gradient:  2.2305625943756695\n",
      "Iteration: 977, Norm of Gradient:  2.190914639374728\n",
      "Iteration: 978, Norm of Gradient:  2.1519718676374238\n",
      "Iteration: 979, Norm of Gradient:  2.113720762078448\n",
      "Iteration: 980, Norm of Gradient:  2.076150279683792\n",
      "Iteration: 981, Norm of Gradient:  2.0392470894349035\n",
      "Iteration: 982, Norm of Gradient:  2.0029997610206443\n",
      "Iteration: 983, Norm of Gradient:  1.9673968524079761\n",
      "Iteration: 984, Norm of Gradient:  1.9324270236381043\n",
      "Iteration: 985, Norm of Gradient:  1.8980784253147327\n",
      "Iteration: 986, Norm of Gradient:  1.864340304744681\n",
      "Iteration: 987, Norm of Gradient:  1.8312019690036747\n",
      "Iteration: 988, Norm of Gradient:  1.7986530842070527\n",
      "Iteration: 989, Norm of Gradient:  1.7666822383304497\n",
      "Iteration: 990, Norm of Gradient:  1.7352798906899571\n",
      "Iteration: 991, Norm of Gradient:  1.7044356744888263\n",
      "Iteration: 992, Norm of Gradient:  1.674139585682117\n",
      "Iteration: 993, Norm of Gradient:  1.644382516604393\n",
      "Iteration: 994, Norm of Gradient:  1.6151535755643789\n",
      "Iteration: 995, Norm of Gradient:  1.586444866938033\n",
      "Iteration: 996, Norm of Gradient:  1.558245520344909\n",
      "Iteration: 997, Norm of Gradient:  1.5305486711394138\n",
      "Iteration: 998, Norm of Gradient:  1.5033432867145649\n",
      "Iteration: 999, Norm of Gradient:  1.476621581105817\n",
      "Iteration: 1000, Norm of Gradient:  1.450375133018897\n",
      "Iteration: 1001, Norm of Gradient:  1.4245948905474852\n",
      "Iteration: 1002, Norm of Gradient:  1.399273150056867\n",
      "Iteration: 1003, Norm of Gradient:  1.3744010989590647\n",
      "Iteration: 1004, Norm of Gradient:  1.3499714644636809\n",
      "Iteration: 1005, Norm of Gradient:  1.3259758372477135\n",
      "Iteration: 1006, Norm of Gradient:  1.3024071968242155\n",
      "Iteration: 1007, Norm of Gradient:  1.2792570120490985\n",
      "Iteration: 1008, Norm of Gradient:  1.2565186314610974\n",
      "Iteration: 1009, Norm of Gradient:  1.2341845418346438\n",
      "Iteration: 1010, Norm of Gradient:  1.2122468503349513\n",
      "Iteration: 1011, Norm of Gradient:  1.19069956577742\n",
      "Iteration: 1012, Norm of Gradient:  1.1695353989872648\n",
      "Iteration: 1013, Norm of Gradient:  1.1487464852574463\n",
      "Iteration: 1014, Norm of Gradient:  1.1283279360376706\n",
      "Iteration: 1015, Norm of Gradient:  1.1082725013871229\n",
      "Iteration: 1016, Norm of Gradient:  1.088573071942707\n",
      "Iteration: 1017, Norm of Gradient:  1.0692241147125607\n",
      "Iteration: 1018, Norm of Gradient:  1.0502183710485544\n",
      "Iteration: 1019, Norm of Gradient:  1.0315509340169149\n",
      "Iteration: 1020, Norm of Gradient:  1.0132159292564722\n",
      "Iteration: 1021, Norm of Gradient:  0.9952056309125056\n",
      "Iteration: 1022, Norm of Gradient:  0.9775166870428085\n",
      "Iteration: 1023, Norm of Gradient:  0.9601411655485871\n",
      "Iteration: 1024, Norm of Gradient:  0.9430749172425742\n",
      "Iteration: 1025, Norm of Gradient:  0.9263119911163994\n",
      "Iteration: 1026, Norm of Gradient:  0.9098472866822327\n",
      "Iteration: 1027, Norm of Gradient:  0.8936747091200625\n",
      "Iteration: 1028, Norm of Gradient:  0.8777898742981284\n",
      "Iteration: 1029, Norm of Gradient:  0.8621870336233883\n",
      "Iteration: 1030, Norm of Gradient:  0.8468617680590704\n",
      "Iteration: 1031, Norm of Gradient:  0.831809111973964\n",
      "Iteration: 1032, Norm of Gradient:  0.8170239713677494\n",
      "Iteration: 1033, Norm of Gradient:  0.802501812703876\n",
      "Iteration: 1034, Norm of Gradient:  0.7882374121119964\n",
      "Iteration: 1035, Norm of Gradient:  0.7742263896653585\n",
      "Iteration: 1036, Norm of Gradient:  0.7604647645390583\n",
      "Iteration: 1037, Norm of Gradient:  0.74694759768312\n",
      "Iteration: 1038, Norm of Gradient:  0.7336708173537947\n",
      "Iteration: 1039, Norm of Gradient:  0.7206301472067744\n",
      "Iteration: 1040, Norm of Gradient:  0.707820832621931\n",
      "Iteration: 1041, Norm of Gradient:  0.6952396157026461\n",
      "Iteration: 1042, Norm of Gradient:  0.6828819850337609\n",
      "Iteration: 1043, Norm of Gradient:  0.6707435600090423\n",
      "Iteration: 1044, Norm of Gradient:  0.6588213312723965\n",
      "Iteration: 1045, Norm of Gradient:  0.6471107948850092\n",
      "Iteration: 1046, Norm of Gradient:  0.6356088972884614\n",
      "Iteration: 1047, Norm of Gradient:  0.6243109371307279\n",
      "Iteration: 1048, Norm of Gradient:  0.6132143099813834\n",
      "Iteration: 1049, Norm of Gradient:  0.6023143543821522\n",
      "Iteration: 1050, Norm of Gradient:  0.5916084769194545\n",
      "Iteration: 1051, Norm of Gradient:  0.5810926875618935\n",
      "Iteration: 1052, Norm of Gradient:  0.570763877360577\n",
      "Iteration: 1053, Norm of Gradient:  0.5606186103793135\n",
      "Iteration: 1054, Norm of Gradient:  0.5506535454961361\n",
      "Iteration: 1055, Norm of Gradient:  0.540865924424893\n",
      "Iteration: 1056, Norm of Gradient:  0.5312522225530772\n",
      "Iteration: 1057, Norm of Gradient:  0.5218094680234079\n",
      "Iteration: 1058, Norm of Gradient:  0.5125342447277159\n",
      "Iteration: 1059, Norm of Gradient:  0.5034242836365684\n",
      "Iteration: 1060, Norm of Gradient:  0.494475815304758\n",
      "Iteration: 1061, Norm of Gradient:  0.4856866726075024\n",
      "Iteration: 1062, Norm of Gradient:  0.47705360995505636\n",
      "Iteration: 1063, Norm of Gradient:  0.46857426130118474\n",
      "Iteration: 1064, Norm of Gradient:  0.46024577494118474\n",
      "Iteration: 1065, Norm of Gradient:  0.45206463741641956\n",
      "Iteration: 1066, Norm of Gradient:  0.4440294644444942\n",
      "Iteration: 1067, Norm of Gradient:  0.4361366559505116\n",
      "Iteration: 1068, Norm of Gradient:  0.4283844034158478\n",
      "Iteration: 1069, Norm of Gradient:  0.42077036994030675\n",
      "Iteration: 1070, Norm of Gradient:  0.4132910815083262\n",
      "Iteration: 1071, Norm of Gradient:  0.4059449915725859\n",
      "Iteration: 1072, Norm of Gradient:  0.3987292539587838\n",
      "Iteration: 1073, Norm of Gradient:  0.391641797696174\n",
      "Iteration: 1074, Norm of Gradient:  0.3846804671349169\n",
      "Iteration: 1075, Norm of Gradient:  0.3778428178399688\n",
      "Iteration: 1076, Norm of Gradient:  0.37112679878465366\n",
      "Iteration: 1077, Norm of Gradient:  0.3645298371324294\n",
      "Iteration: 1078, Norm of Gradient:  0.3580505668902582\n",
      "Iteration: 1079, Norm of Gradient:  0.35168629387840206\n",
      "Iteration: 1080, Norm of Gradient:  0.3454352200415466\n",
      "Iteration: 1081, Norm of Gradient:  0.33929520458588036\n",
      "Iteration: 1082, Norm of Gradient:  0.33326452396250705\n",
      "Iteration: 1083, Norm of Gradient:  0.32734083591894125\n",
      "Iteration: 1084, Norm of Gradient:  0.3215221237686561\n",
      "Iteration: 1085, Norm of Gradient:  0.31580686137069075\n",
      "Iteration: 1086, Norm of Gradient:  0.31019355023603823\n",
      "Iteration: 1087, Norm of Gradient:  0.30468034415738354\n",
      "Iteration: 1088, Norm of Gradient:  0.29926492348073\n",
      "Iteration: 1089, Norm of Gradient:  0.2939452246896642\n",
      "Iteration: 1090, Norm of Gradient:  0.288719991485354\n",
      "Iteration: 1091, Norm of Gradient:  0.2835882519762343\n",
      "Iteration: 1092, Norm of Gradient:  0.2785478607407769\n",
      "Iteration: 1093, Norm of Gradient:  0.2735963843800929\n",
      "Iteration: 1094, Norm of Gradient:  0.2687334437989949\n",
      "Iteration: 1095, Norm of Gradient:  0.2639571311270417\n",
      "Iteration: 1096, Norm of Gradient:  0.25926544192988205\n",
      "Iteration: 1097, Norm of Gradient:  0.25465652610171036\n",
      "Iteration: 1098, Norm of Gradient:  0.25013005620257556\n",
      "Iteration: 1099, Norm of Gradient:  0.2456841413078233\n",
      "Iteration: 1100, Norm of Gradient:  0.2413168235486273\n",
      "Iteration: 1101, Norm of Gradient:  0.2370279751546161\n",
      "Iteration: 1102, Norm of Gradient:  0.23281495488875636\n",
      "Iteration: 1103, Norm of Gradient:  0.22867648669179164\n",
      "Iteration: 1104, Norm of Gradient:  0.22461149598358005\n",
      "Iteration: 1105, Norm of Gradient:  0.22061944375326897\n",
      "Iteration: 1106, Norm of Gradient:  0.21669803019798864\n",
      "Iteration: 1107, Norm of Gradient:  0.21284628195391223\n",
      "Iteration: 1108, Norm of Gradient:  0.2090627687607977\n",
      "Iteration: 1109, Norm of Gradient:  0.2053469988584669\n",
      "Iteration: 1110, Norm of Gradient:  0.20169663271510566\n",
      "Iteration: 1111, Norm of Gradient:  0.1981114845655891\n",
      "Iteration: 1112, Norm of Gradient:  0.19459037355901168\n",
      "Iteration: 1113, Norm of Gradient:  0.19113183090430363\n",
      "Iteration: 1114, Norm of Gradient:  0.18773420210428668\n",
      "Iteration: 1115, Norm of Gradient:  0.18439709962110673\n",
      "Iteration: 1116, Norm of Gradient:  0.18111985110067536\n",
      "Iteration: 1117, Norm of Gradient:  0.1779003288031929\n",
      "Iteration: 1118, Norm of Gradient:  0.17473791318688306\n",
      "Iteration: 1119, Norm of Gradient:  0.171632426656806\n",
      "Iteration: 1120, Norm of Gradient:  0.1685815165530262\n",
      "Iteration: 1121, Norm of Gradient:  0.1655848209303182\n",
      "Iteration: 1122, Norm of Gradient:  0.1626415946103099\n",
      "Iteration: 1123, Norm of Gradient:  0.1597510357951943\n",
      "Iteration: 1124, Norm of Gradient:  0.15691113428989914\n",
      "Iteration: 1125, Norm of Gradient:  0.15412217895790153\n",
      "Iteration: 1126, Norm of Gradient:  0.15138247949288172\n",
      "Iteration: 1127, Norm of Gradient:  0.1486918638832576\n",
      "Iteration: 1128, Norm of Gradient:  0.14604890760087483\n",
      "Iteration: 1129, Norm of Gradient:  0.14345328332055318\n",
      "Iteration: 1130, Norm of Gradient:  0.14090306608254682\n",
      "Iteration: 1131, Norm of Gradient:  0.1383983359753041\n",
      "Iteration: 1132, Norm of Gradient:  0.13593870266639912\n",
      "Iteration: 1133, Norm of Gradient:  0.13352252634650727\n",
      "Iteration: 1134, Norm of Gradient:  0.13114886645814144\n",
      "Iteration: 1135, Norm of Gradient:  0.12881778039865532\n",
      "Iteration: 1136, Norm of Gradient:  0.12652800588253763\n",
      "Iteration: 1137, Norm of Gradient:  0.12427898193693444\n",
      "Iteration: 1138, Norm of Gradient:  0.12207049734337999\n",
      "Iteration: 1139, Norm of Gradient:  0.11990039084340598\n",
      "Iteration: 1140, Norm of Gradient:  0.1177689757289317\n",
      "Iteration: 1141, Norm of Gradient:  0.11567585908226881\n",
      "Iteration: 1142, Norm of Gradient:  0.11361938517399106\n",
      "Iteration: 1143, Norm of Gradient:  0.11160005113687704\n",
      "Iteration: 1144, Norm of Gradient:  0.10961615915457566\n",
      "Iteration: 1145, Norm of Gradient:  0.10766785235493484\n",
      "Iteration: 1146, Norm of Gradient:  0.10575435187588642\n",
      "Iteration: 1147, Norm of Gradient:  0.10387425427204908\n",
      "Iteration: 1148, Norm of Gradient:  0.10202792844937193\n",
      "Iteration: 1149, Norm of Gradient:  0.10021469784570922\n",
      "Iteration: 1150, Norm of Gradient:  0.09843310688096671\n",
      "Iteration: 1151, Norm of Gradient:  0.09668367040637375\n",
      "Iteration: 1152, Norm of Gradient:  0.09496532626310476\n",
      "Iteration: 1153, Norm of Gradient:  0.0932774311999237\n",
      "Iteration: 1154, Norm of Gradient:  0.09161940681235245\n",
      "Iteration: 1155, Norm of Gradient:  0.08999040636040403\n",
      "Iteration: 1156, Norm of Gradient:  0.08839116081382202\n",
      "Iteration: 1157, Norm of Gradient:  0.08681992009771602\n",
      "Iteration: 1158, Norm of Gradient:  0.08527682827791976\n",
      "Iteration: 1159, Norm of Gradient:  0.08376126945680466\n",
      "Iteration: 1160, Norm of Gradient:  0.08227260399440128\n",
      "Iteration: 1161, Norm of Gradient:  0.08080970373416917\n",
      "Iteration: 1162, Norm of Gradient:  0.07937334826618454\n",
      "Iteration: 1163, Norm of Gradient:  0.07796262574435875\n",
      "Iteration: 1164, Norm of Gradient:  0.07657684162072652\n",
      "Iteration: 1165, Norm of Gradient:  0.07521553516173361\n",
      "Iteration: 1166, Norm of Gradient:  0.07387886342625144\n",
      "Iteration: 1167, Norm of Gradient:  0.0725654418392564\n",
      "Iteration: 1168, Norm of Gradient:  0.07127557430803676\n",
      "Iteration: 1169, Norm of Gradient:  0.07000873303914923\n",
      "Iteration: 1170, Norm of Gradient:  0.06876432539461448\n",
      "Iteration: 1171, Norm of Gradient:  0.06754220268030318\n",
      "Iteration: 1172, Norm of Gradient:  0.06634178243412536\n",
      "Iteration: 1173, Norm of Gradient:  0.06516212031923271\n",
      "Iteration: 1174, Norm of Gradient:  0.06400399087840486\n",
      "Iteration: 1175, Norm of Gradient:  0.06286670135800586\n",
      "Iteration: 1176, Norm of Gradient:  0.06174893851143437\n",
      "Iteration: 1177, Norm of Gradient:  0.06065141129164228\n",
      "Iteration: 1178, Norm of Gradient:  0.059573409067667185\n",
      "Iteration: 1179, Norm of Gradient:  0.05851463227053986\n",
      "Iteration: 1180, Norm of Gradient:  0.0574745384324391\n",
      "Iteration: 1181, Norm of Gradient:  0.05645284136952542\n",
      "Iteration: 1182, Norm of Gradient:  0.05544927212872395\n",
      "Iteration: 1183, Norm of Gradient:  0.054464100594723494\n",
      "Iteration: 1184, Norm of Gradient:  0.053495456186521656\n",
      "Iteration: 1185, Norm of Gradient:  0.052544897727735886\n",
      "Iteration: 1186, Norm of Gradient:  0.05161074285987552\n",
      "Iteration: 1187, Norm of Gradient:  0.050693308269070717\n",
      "Iteration: 1188, Norm of Gradient:  0.04979258484771494\n",
      "Iteration: 1189, Norm of Gradient:  0.048907226944028936\n",
      "Iteration: 1190, Norm of Gradient:  0.04803808308813337\n",
      "Iteration: 1191, Norm of Gradient:  0.047184295602151684\n",
      "Iteration: 1192, Norm of Gradient:  0.046345235233484314\n",
      "Iteration: 1193, Norm of Gradient:  0.045521568931435163\n",
      "Iteration: 1194, Norm of Gradient:  0.044712738548439795\n",
      "Iteration: 1195, Norm of Gradient:  0.043917699431953956\n",
      "Iteration: 1196, Norm of Gradient:  0.04313673805793256\n",
      "Iteration: 1197, Norm of Gradient:  0.04237041606843754\n",
      "Iteration: 1198, Norm of Gradient:  0.041617564357009\n",
      "Iteration: 1199, Norm of Gradient:  0.0408776820665765\n",
      "Iteration: 1200, Norm of Gradient:  0.040151223470270156\n",
      "Iteration: 1201, Norm of Gradient:  0.03943740857502257\n",
      "Iteration: 1202, Norm of Gradient:  0.038736653853703916\n",
      "Iteration: 1203, Norm of Gradient:  0.03804771408641883\n",
      "Iteration: 1204, Norm of Gradient:  0.03737118689386093\n",
      "Iteration: 1205, Norm of Gradient:  0.036707109722830436\n",
      "Iteration: 1206, Norm of Gradient:  0.03605458746540761\n",
      "Iteration: 1207, Norm of Gradient:  0.035413738941769676\n",
      "Iteration: 1208, Norm of Gradient:  0.03478463496736204\n",
      "Iteration: 1209, Norm of Gradient:  0.03416599202772255\n",
      "Iteration: 1210, Norm of Gradient:  0.03355893565945877\n",
      "Iteration: 1211, Norm of Gradient:  0.03296242259708068\n",
      "Iteration: 1212, Norm of Gradient:  0.032376478983819935\n",
      "Iteration: 1213, Norm of Gradient:  0.03180113163949477\n",
      "Iteration: 1214, Norm of Gradient:  0.031235627998862748\n",
      "Iteration: 1215, Norm of Gradient:  0.030680303857489256\n",
      "Iteration: 1216, Norm of Gradient:  0.03013547122013489\n",
      "Iteration: 1217, Norm of Gradient:  0.02959951096456273\n",
      "Iteration: 1218, Norm of Gradient:  0.029073171277614458\n",
      "Iteration: 1219, Norm of Gradient:  0.028556724080226623\n",
      "Iteration: 1220, Norm of Gradient:  0.0280491124167803\n",
      "Iteration: 1221, Norm of Gradient:  0.02755051332923794\n",
      "Iteration: 1222, Norm of Gradient:  0.02706083302951709\n",
      "Iteration: 1223, Norm of Gradient:  0.02657978997569267\n",
      "Iteration: 1224, Norm of Gradient:  0.026107374923978858\n",
      "Iteration: 1225, Norm of Gradient:  0.025643268409410218\n",
      "Iteration: 1226, Norm of Gradient:  0.025187284473737567\n",
      "Iteration: 1227, Norm of Gradient:  0.024739573755337845\n",
      "Iteration: 1228, Norm of Gradient:  0.02430028872739846\n",
      "Iteration: 1229, Norm of Gradient:  0.023868066788520773\n",
      "Iteration: 1230, Norm of Gradient:  0.023443569626479907\n",
      "Iteration: 1231, Norm of Gradient:  0.023027162719277794\n",
      "Iteration: 1232, Norm of Gradient:  0.022617783585502583\n",
      "Iteration: 1233, Norm of Gradient:  0.022215835982189413\n",
      "Iteration: 1234, Norm of Gradient:  0.021820822820461588\n",
      "Iteration: 1235, Norm of Gradient:  0.021433257815547002\n",
      "Iteration: 1236, Norm of Gradient:  0.02105223962723867\n",
      "Iteration: 1237, Norm of Gradient:  0.020677692103062598\n",
      "Iteration: 1238, Norm of Gradient:  0.020310155288393215\n",
      "Iteration: 1239, Norm of Gradient:  0.019949081256663767\n",
      "Iteration: 1240, Norm of Gradient:  0.019594967358981457\n",
      "Iteration: 1241, Norm of Gradient:  0.019246618163370602\n",
      "Iteration: 1242, Norm of Gradient:  0.018904549793827483\n",
      "Iteration: 1243, Norm of Gradient:  0.018568400270442053\n",
      "Iteration: 1244, Norm of Gradient:  0.01823835159188191\n",
      "Iteration: 1245, Norm of Gradient:  0.017914059434072652\n",
      "Iteration: 1246, Norm of Gradient:  0.017595319312167033\n",
      "Iteration: 1247, Norm of Gradient:  0.017282645315283236\n",
      "Iteration: 1248, Norm of Gradient:  0.016975495017341895\n",
      "Iteration: 1249, Norm of Gradient:  0.01667411707826624\n",
      "Iteration: 1250, Norm of Gradient:  0.016377184524408825\n",
      "Iteration: 1251, Norm of Gradient:  0.01608631761464524\n",
      "Iteration: 1252, Norm of Gradient:  0.01580066052514127\n",
      "Iteration: 1253, Norm of Gradient:  0.015519843219842839\n",
      "Iteration: 1254, Norm of Gradient:  0.015243906263996123\n",
      "Iteration: 1255, Norm of Gradient:  0.01497286102794607\n",
      "Iteration: 1256, Norm of Gradient:  0.01470690195679898\n",
      "Iteration: 1257, Norm of Gradient:  0.014445339151228764\n",
      "Iteration: 1258, Norm of Gradient:  0.014188941147620586\n",
      "Iteration: 1259, Norm of Gradient:  0.01393668743200134\n",
      "Iteration: 1260, Norm of Gradient:  0.01368869767872907\n",
      "Iteration: 1261, Norm of Gradient:  0.013445364581990736\n",
      "Iteration: 1262, Norm of Gradient:  0.013206155360755891\n",
      "Iteration: 1263, Norm of Gradient:  0.0129720944254191\n",
      "Iteration: 1264, Norm of Gradient:  0.012741526614139333\n",
      "Iteration: 1265, Norm of Gradient:  0.012514799363362139\n",
      "Iteration: 1266, Norm of Gradient:  0.012292353937689404\n",
      "Iteration: 1267, Norm of Gradient:  0.012073746284619326\n",
      "Iteration: 1268, Norm of Gradient:  0.011858734488612438\n",
      "Iteration: 1269, Norm of Gradient:  0.01164781988058185\n",
      "Iteration: 1270, Norm of Gradient:  0.011441078851162915\n",
      "Iteration: 1271, Norm of Gradient:  0.011238041552783852\n",
      "Iteration: 1272, Norm of Gradient:  0.011038535382743941\n",
      "Iteration: 1273, Norm of Gradient:  0.010841592750481943\n",
      "Iteration: 1274, Norm of Gradient:  0.010648806677236574\n",
      "Iteration: 1275, Norm of Gradient:  0.010459620350821118\n",
      "Iteration: 1276, Norm of Gradient:  0.010274029584382295\n",
      "Iteration: 1277, Norm of Gradient:  0.010091394523078402\n",
      "Iteration: 1278, Norm of Gradient:  0.009911748086495682\n",
      "R-Squared:   0.51735\n"
     ]
    }
   ],
   "source": [
    "# Repeat part c)\n",
    "beta,returned_MSE_list, y, y_pred, standardizer = gd(train_data[features_w_interaction],y = y_train, learning_parameter = .00004, tau = .01)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "#training r2\n",
    "training_r2 = rsquared(y_true = y.flatten(), y_pred = y_pred.flatten())\n",
    "\n",
    "#generate predictions\n",
    "X_test = StandardScaler().fit_transform(test_data[features_w_interaction])\n",
    "test_predictions = X_test @ beta\n",
    "\n",
    "test_r2 = rsquared(y_true = y_test.to_numpy().flatten(), y_pred= test_predictions.flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training R2 from Gradient Descent Regression Implementation (additional features):   0.51735\n",
      "Testing R2 from Gradient Descent Regression Implementation (additional features):   0.51083\n"
     ]
    }
   ],
   "source": [
    "# Repeat a)\n",
    "print(\"Training R2 from Gradient Descent Regression Implementation (additional features):   {:.5f}\".format(training_r2))\n",
    "print(\"Testing R2 from Gradient Descent Regression Implementation (additional features):   {:.5f}\".format(test_r2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Part e**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "39d961a08718555772bb0ac411a6b2b438a184f9ac0d65f0613f436450256741"
  },
  "kernelspec": {
   "display_name": "Python 3.7.4 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
